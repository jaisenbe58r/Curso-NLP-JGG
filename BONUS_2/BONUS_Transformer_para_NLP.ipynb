{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BONUS Transformer_para NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BQN8jwx48_yU",
        "-SBoH8G4XyR9"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9JJ7FBw84tG",
        "colab_type": "text"
      },
      "source": [
        "# Fase 1: Importar las dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5DbIHC-F6Hf",
        "colab_type": "text"
      },
      "source": [
        "**Paper original**: All you need is Attention https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbcvtPlp3YWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6o_cpZz3y_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQN8jwx48_yU",
        "colab_type": "text"
      },
      "source": [
        "# Fase 2: Pre Procesado de Datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPlOT-2mlw0r",
        "colab_type": "text"
      },
      "source": [
        "## Carga de Ficheros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCD9jwXsLwS_",
        "colab_type": "text"
      },
      "source": [
        "Importamos los ficheros de nuestro Google Drive personal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQpbl1pXCR0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "1b754e89-8ee9-4b39-951e-ca4964cda8e5"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8Or0sLV5b8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/Curso de NLP/Transformer/data/europarl-v7.es-en.en\", \n",
        "          mode = \"r\", encoding = \"utf-8\") as f:\n",
        "    europarl_en = f.read()\n",
        "with open(\"/content/drive/My Drive/Curso de NLP/Transformer/data/europarl-v7.es-en.es\", \n",
        "          mode = \"r\", encoding = \"utf-8\") as f:\n",
        "    europarl_es = f.read()\n",
        "with open(\"/content/drive/My Drive/Curso de NLP/Transformer/data/nonbreaking_prefix.en\", \n",
        "          mode = \"r\", encoding = \"utf-8\") as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/My Drive/Curso de NLP/Transformer/data/nonbreaking_prefix.en\", \n",
        "          mode = \"r\", encoding = \"utf-8\") as f:\n",
        "    non_breaking_prefix_es = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMAFFdpIyNZd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "eb684c33-6895-4fc9-a27d-420c05a5fab8"
      },
      "source": [
        "europarl_en[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Resumption of the session\\nI declare resumed the session of the European Parliament adjourned on Frid'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYgCMq6myYIi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "f76b75fd-25df-43f2-fe4b-f88de485e69d"
      },
      "source": [
        "europarl_es[:100]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Reanudación del período de sesiones\\nDeclaro reanudado el período de sesiones del Parlamento Europeo,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEFw0D2vP_Dl",
        "colab_type": "text"
      },
      "source": [
        "## Limpieza de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwIBeGXn7LIJ",
        "colab_type": "text"
      },
      "source": [
        "Vamos a obtener los non_breaking_prefixes como una lista de palabras limpias con un punto al final para que nos sea más fácil de utilizar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_TeuktU40Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_es = non_breaking_prefix_es.split(\"\\n\")\n",
        "non_breaking_prefix_es = [' ' + pref + '.' for pref in non_breaking_prefix_es]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9x4mZfKMaxD",
        "colab_type": "text"
      },
      "source": [
        "Necesitaremos cada palabra y otro símbolo que queramos mantener en minúsculas y separados por espacios para que podamos \"tokenizarlos\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg-8LLK-WdFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en\n",
        "# Añadimos $$$ después de los puntos de frases sin fin\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "# Eliminamos los marcadores $$$\n",
        "corpus_en = re.sub(r\"\\.\\$\\$\\$\", '', corpus_en)\n",
        "# Eliminamos espacios múltiples\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "corpus_es = europarl_es\n",
        "for prefix in non_breaking_prefix_es:\n",
        "    corpus_es = corpus_es.replace(prefix, prefix + '$$$')\n",
        "corpus_es = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_es)\n",
        "corpus_es = re.sub(r\"\\.\\$\\$\\$\", '', corpus_es)\n",
        "corpus_es = re.sub(r\"  +\", \" \", corpus_es)\n",
        "corpus_es = corpus_es.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaTONZ-0OqoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "corpus_en=pd.read_csv(\"/content/drive/My Drive/Curso de NLP/Transformer/data/corpus_en.csv\").values\n",
        "corpus_es=pd.read_csv(\"/content/drive/My Drive/Curso de NLP/Transformer/data/corpus_es.csv\").values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-Y9v8-Tozl2",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizar el Texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5YXanmOd_xK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "tokenizer_es = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_es, target_vocab_size=2**13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftIbPzIwCtwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 # = 8198\n",
        "VOCAB_SIZE_ES = tokenizer_es.vocab_size + 2 # = 8225"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPFe2YJDC9jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_ES-2] + tokenizer_es.encode(sentence) + [VOCAB_SIZE_ES-1]\n",
        "           for sentence in corpus_es]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG6AlcFMpC5C",
        "colab_type": "text"
      },
      "source": [
        "## Eliminamos las frases demasiado largas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6CD6PLGyQWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjKXjc92WkyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(inputs).to_csv(\"/content/drive/My Drive/Curso de NLP/Transformer/data/inputs.csv\", index=False)\n",
        "pd.DataFrame(outputs).to_csv(\"/content/drive/My Drive/Curso de NLP/Transformer/data/outputs.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypm8h5aZQTZ1",
        "colab_type": "text"
      },
      "source": [
        "## Creamos las entradas y las salidas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FP0WPsdM8hl",
        "colab_type": "text"
      },
      "source": [
        "A medida que entrenamos con bloques, necesitaremos que cada entrada tenga la misma longitud. Rellenamos con el token apropiado, y nos aseguraremos de que este token de relleno no interfiera con nuestro entrenamiento más adelante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvDfLDWUONlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFxMp3TOIYff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycT0YqydRcUd",
        "colab_type": "text"
      },
      "source": [
        "# Fase 3: Construcción del Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SBoH8G4XyR9",
        "colab_type": "text"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G9C3ucmJ86I",
        "colab_type": "text"
      },
      "source": [
        "Fórmula de la Codificación Posicional:\n",
        "\n",
        "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
        "\n",
        "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2wc6sYlX0dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "    \n",
        "    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles # (seq_length, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcw8YIQqRhOJ",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sffhwwvX-wj",
        "colab_type": "text"
      },
      "source": [
        "### Cálculo de la Atención"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VBuW6lESLDX",
        "colab_type": "text"
      },
      "source": [
        "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rEoCNJURbrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention_weights = tf.nn.softmax(scaled_product, axis=-1)\n",
        "\n",
        "    attention = tf.matmul(attention_weights, values)\n",
        "    \n",
        "    return attention, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MjtvXrfYEx7",
        "colab_type": "text"
      },
      "source": [
        "### Sub capa de atención de encabezado múltiple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvq4I9uTX5p7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention, attention_weights = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiyuHe1OeT5N",
        "colab_type": "text"
      },
      "source": [
        "## Codificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV0ZMH7KT_KZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        attention, _ = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-P92KeZih60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DthraBEwuvl",
        "colab_type": "text"
      },
      "source": [
        "## Descodificación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZWZyFBnwy8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combinado con la salida del encoder \n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention, attention_weights_1 = self.multi_head_attention_1(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        attention_2, attention_weights_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs, attention_weights_1, attention_weights_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpzdiWHiwywF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        att_weights = {}\n",
        "\n",
        "        for i in range(self.nb_layers):\n",
        "            outputs, att_weights_self, att_weights_cross = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "            att_weights[\"decoder_layer_{}_self\".format(i+1)] = att_weights_self\n",
        "            att_weights[\"decoder_layer_{}_cross\".format(i+1)] = att_weights_cross\n",
        "\n",
        "        return outputs, att_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5sJYkjbz5DD",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqvqNjJPwyh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "    \n",
        "    def create_padding_mask(self, seq): #seq: (batch_size, seq_length)\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs, att_weights = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs, att_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c-LRThUPrso",
        "colab_type": "text"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiOdqQ5qPs8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hiper Parámetros\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_ES,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46xg4Wrg1Wgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Goque362343",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb_32PIU5Zkh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6412cfd7-9a9f-44f3-e1ed-f71158e18ff5"
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/Curso de NLP/Transformer/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Último checkpoint restaurado!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhFK5kUx602K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb4e7c4b-c112-478e-f76c-8ab979724455"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Inicio del epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions, _ = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Lote {} Pérdida {:.4f} Precisión {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Guardando checkpoint para el epoch {} en {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Tiempo que ha tardado 1 epoch: {} segs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inicio del epoch 1\n",
            "Epoch 1 Lote 0 Pérdida 5.7712 Precisión 0.0000\n",
            "Epoch 1 Lote 50 Pérdida 6.0703 Precisión 0.0008\n",
            "Epoch 1 Lote 100 Pérdida 6.0209 Precisión 0.0249\n",
            "Epoch 1 Lote 150 Pérdida 5.9672 Precisión 0.0341\n",
            "Epoch 1 Lote 200 Pérdida 5.8880 Precisión 0.0388\n",
            "Epoch 1 Lote 250 Pérdida 5.7829 Precisión 0.0431\n",
            "Epoch 1 Lote 300 Pérdida 5.6780 Precisión 0.0512\n",
            "Epoch 1 Lote 350 Pérdida 5.5599 Precisión 0.0575\n",
            "Epoch 1 Lote 400 Pérdida 5.4499 Precisión 0.0622\n",
            "Epoch 1 Lote 450 Pérdida 5.3427 Precisión 0.0662\n",
            "Epoch 1 Lote 500 Pérdida 5.2409 Precisión 0.0704\n",
            "Epoch 1 Lote 550 Pérdida 5.1469 Precisión 0.0750\n",
            "Epoch 1 Lote 600 Pérdida 5.0560 Precisión 0.0796\n",
            "Epoch 1 Lote 650 Pérdida 4.9704 Precisión 0.0841\n",
            "Epoch 1 Lote 700 Pérdida 4.8874 Precisión 0.0882\n",
            "Epoch 1 Lote 750 Pérdida 4.8114 Precisión 0.0924\n",
            "Epoch 1 Lote 800 Pérdida 4.7401 Precisión 0.0967\n",
            "Epoch 1 Lote 850 Pérdida 4.6730 Precisión 0.1008\n",
            "Epoch 1 Lote 900 Pérdida 4.6113 Precisión 0.1048\n",
            "Epoch 1 Lote 950 Pérdida 4.5494 Precisión 0.1086\n",
            "Epoch 1 Lote 1000 Pérdida 4.4932 Precisión 0.1124\n",
            "Epoch 1 Lote 1050 Pérdida 4.4374 Precisión 0.1158\n",
            "Epoch 1 Lote 1100 Pérdida 4.3844 Precisión 0.1191\n",
            "Epoch 1 Lote 1150 Pérdida 4.3329 Precisión 0.1224\n",
            "Epoch 1 Lote 1200 Pérdida 4.2865 Precisión 0.1255\n",
            "Epoch 1 Lote 1250 Pérdida 4.2397 Precisión 0.1285\n",
            "Epoch 1 Lote 1300 Pérdida 4.1947 Precisión 0.1312\n",
            "Epoch 1 Lote 1350 Pérdida 4.1520 Precisión 0.1339\n",
            "Epoch 1 Lote 1400 Pérdida 4.1133 Precisión 0.1366\n",
            "Epoch 1 Lote 1450 Pérdida 4.0745 Precisión 0.1392\n",
            "Epoch 1 Lote 1500 Pérdida 4.0383 Precisión 0.1416\n",
            "Epoch 1 Lote 1550 Pérdida 4.0026 Precisión 0.1439\n",
            "Epoch 1 Lote 1600 Pérdida 3.9673 Precisión 0.1462\n",
            "Epoch 1 Lote 1650 Pérdida 3.9335 Precisión 0.1485\n",
            "Epoch 1 Lote 1700 Pérdida 3.9013 Precisión 0.1508\n",
            "Epoch 1 Lote 1750 Pérdida 3.8685 Precisión 0.1529\n",
            "Epoch 1 Lote 1800 Pérdida 3.8372 Precisión 0.1549\n",
            "Epoch 1 Lote 1850 Pérdida 3.8068 Precisión 0.1569\n",
            "Epoch 1 Lote 1900 Pérdida 3.7779 Precisión 0.1587\n",
            "Epoch 1 Lote 1950 Pérdida 3.7506 Precisión 0.1605\n",
            "Epoch 1 Lote 2000 Pérdida 3.7238 Precisión 0.1624\n",
            "Epoch 1 Lote 2050 Pérdida 3.6972 Precisión 0.1642\n",
            "Epoch 1 Lote 2100 Pérdida 3.6713 Precisión 0.1661\n",
            "Epoch 1 Lote 2150 Pérdida 3.6463 Precisión 0.1679\n",
            "Epoch 1 Lote 2200 Pérdida 3.6211 Precisión 0.1698\n",
            "Epoch 1 Lote 2250 Pérdida 3.5963 Precisión 0.1715\n",
            "Epoch 1 Lote 2300 Pérdida 3.5718 Precisión 0.1733\n",
            "Epoch 1 Lote 2350 Pérdida 3.5482 Precisión 0.1751\n",
            "Epoch 1 Lote 2400 Pérdida 3.5251 Precisión 0.1768\n",
            "Epoch 1 Lote 2450 Pérdida 3.5016 Precisión 0.1786\n",
            "Epoch 1 Lote 2500 Pérdida 3.4787 Precisión 0.1805\n",
            "Epoch 1 Lote 2550 Pérdida 3.4551 Precisión 0.1822\n",
            "Epoch 1 Lote 2600 Pérdida 3.4322 Precisión 0.1841\n",
            "Epoch 1 Lote 2650 Pérdida 3.4102 Precisión 0.1859\n",
            "Epoch 1 Lote 2700 Pérdida 3.3890 Precisión 0.1878\n",
            "Epoch 1 Lote 2750 Pérdida 3.3689 Precisión 0.1897\n",
            "Epoch 1 Lote 2800 Pérdida 3.3484 Precisión 0.1916\n",
            "Epoch 1 Lote 2850 Pérdida 3.3280 Precisión 0.1934\n",
            "Epoch 1 Lote 2900 Pérdida 3.3086 Precisión 0.1952\n",
            "Epoch 1 Lote 2950 Pérdida 3.2898 Precisión 0.1970\n",
            "Epoch 1 Lote 3000 Pérdida 3.2707 Precisión 0.1990\n",
            "Epoch 1 Lote 3050 Pérdida 3.2522 Precisión 0.2009\n",
            "Epoch 1 Lote 3100 Pérdida 3.2344 Precisión 0.2028\n",
            "Epoch 1 Lote 3150 Pérdida 3.2168 Precisión 0.2046\n",
            "Epoch 1 Lote 3200 Pérdida 3.1991 Precisión 0.2065\n",
            "Epoch 1 Lote 3250 Pérdida 3.1820 Precisión 0.2083\n",
            "Epoch 1 Lote 3300 Pérdida 3.1649 Precisión 0.2102\n",
            "Epoch 1 Lote 3350 Pérdida 3.1481 Precisión 0.2120\n",
            "Epoch 1 Lote 3400 Pérdida 3.1314 Precisión 0.2138\n",
            "Epoch 1 Lote 3450 Pérdida 3.1151 Precisión 0.2156\n",
            "Epoch 1 Lote 3500 Pérdida 3.0987 Precisión 0.2174\n",
            "Epoch 1 Lote 3550 Pérdida 3.0830 Precisión 0.2192\n",
            "Epoch 1 Lote 3600 Pérdida 3.0669 Precisión 0.2208\n",
            "Epoch 1 Lote 3650 Pérdida 3.0511 Precisión 0.2225\n",
            "Epoch 1 Lote 3700 Pérdida 3.0356 Precisión 0.2242\n",
            "Epoch 1 Lote 3750 Pérdida 3.0201 Precisión 0.2259\n",
            "Epoch 1 Lote 3800 Pérdida 3.0056 Precisión 0.2276\n",
            "Epoch 1 Lote 3850 Pérdida 2.9907 Precisión 0.2292\n",
            "Epoch 1 Lote 3900 Pérdida 2.9762 Precisión 0.2309\n",
            "Epoch 1 Lote 3950 Pérdida 2.9616 Precisión 0.2325\n",
            "Epoch 1 Lote 4000 Pérdida 2.9472 Precisión 0.2341\n",
            "Epoch 1 Lote 4050 Pérdida 2.9330 Precisión 0.2358\n",
            "Epoch 1 Lote 4100 Pérdida 2.9189 Precisión 0.2374\n",
            "Epoch 1 Lote 4150 Pérdida 2.9043 Precisión 0.2391\n",
            "Epoch 1 Lote 4200 Pérdida 2.8902 Precisión 0.2408\n",
            "Epoch 1 Lote 4250 Pérdida 2.8767 Precisión 0.2424\n",
            "Epoch 1 Lote 4300 Pérdida 2.8634 Precisión 0.2441\n",
            "Epoch 1 Lote 4350 Pérdida 2.8503 Precisión 0.2457\n",
            "Epoch 1 Lote 4400 Pérdida 2.8371 Precisión 0.2473\n",
            "Epoch 1 Lote 4450 Pérdida 2.8239 Precisión 0.2489\n",
            "Epoch 1 Lote 4500 Pérdida 2.8110 Precisión 0.2504\n",
            "Epoch 1 Lote 4550 Pérdida 2.7981 Precisión 0.2520\n",
            "Epoch 1 Lote 4600 Pérdida 2.7854 Precisión 0.2536\n",
            "Epoch 1 Lote 4650 Pérdida 2.7728 Precisión 0.2551\n",
            "Epoch 1 Lote 4700 Pérdida 2.7614 Precisión 0.2565\n",
            "Epoch 1 Lote 4750 Pérdida 2.7508 Precisión 0.2578\n",
            "Epoch 1 Lote 4800 Pérdida 2.7407 Precisión 0.2590\n",
            "Epoch 1 Lote 4850 Pérdida 2.7307 Precisión 0.2601\n",
            "Epoch 1 Lote 4900 Pérdida 2.7215 Precisión 0.2612\n",
            "Epoch 1 Lote 4950 Pérdida 2.7125 Precisión 0.2622\n",
            "Epoch 1 Lote 5000 Pérdida 2.7041 Precisión 0.2632\n",
            "Epoch 1 Lote 5050 Pérdida 2.6956 Precisión 0.2641\n",
            "Epoch 1 Lote 5100 Pérdida 2.6872 Precisión 0.2650\n",
            "Epoch 1 Lote 5150 Pérdida 2.6791 Precisión 0.2659\n",
            "Epoch 1 Lote 5200 Pérdida 2.6708 Precisión 0.2667\n",
            "Epoch 1 Lote 5250 Pérdida 2.6632 Precisión 0.2675\n",
            "Epoch 1 Lote 5300 Pérdida 2.6554 Precisión 0.2683\n",
            "Epoch 1 Lote 5350 Pérdida 2.6480 Precisión 0.2692\n",
            "Epoch 1 Lote 5400 Pérdida 2.6405 Precisión 0.2700\n",
            "Epoch 1 Lote 5450 Pérdida 2.6328 Precisión 0.2708\n",
            "Epoch 1 Lote 5500 Pérdida 2.6254 Precisión 0.2716\n",
            "Epoch 1 Lote 5550 Pérdida 2.6182 Precisión 0.2725\n",
            "Epoch 1 Lote 5600 Pérdida 2.6109 Precisión 0.2733\n",
            "Epoch 1 Lote 5650 Pérdida 2.6037 Precisión 0.2740\n",
            "Epoch 1 Lote 5700 Pérdida 2.5967 Precisión 0.2747\n",
            "Epoch 1 Lote 5750 Pérdida 2.5896 Precisión 0.2755\n",
            "Epoch 1 Lote 5800 Pérdida 2.5829 Precisión 0.2763\n",
            "Epoch 1 Lote 5850 Pérdida 2.5760 Precisión 0.2770\n",
            "Epoch 1 Lote 5900 Pérdida 2.5691 Precisión 0.2777\n",
            "Epoch 1 Lote 5950 Pérdida 2.5625 Precisión 0.2784\n",
            "Epoch 1 Lote 6000 Pérdida 2.5556 Precisión 0.2790\n",
            "Epoch 1 Lote 6050 Pérdida 2.5487 Precisión 0.2797\n",
            "Epoch 1 Lote 6100 Pérdida 2.5418 Precisión 0.2804\n",
            "Epoch 1 Lote 6150 Pérdida 2.5352 Precisión 0.2810\n",
            "Epoch 1 Lote 6200 Pérdida 2.5286 Precisión 0.2817\n",
            "Epoch 1 Lote 6250 Pérdida 2.5220 Precisión 0.2824\n",
            "Epoch 1 Lote 6300 Pérdida 2.5156 Precisión 0.2831\n",
            "Epoch 1 Lote 6350 Pérdida 2.5088 Precisión 0.2837\n",
            "Epoch 1 Lote 6400 Pérdida 2.5024 Precisión 0.2844\n",
            "Guardando checkpoint para el epoch 1 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-1\n",
            "Tiempo que ha tardado 1 epoch: 1600.0238528251648 segs\n",
            "\n",
            "Inicio del epoch 2\n",
            "Epoch 2 Lote 0 Pérdida 1.7629 Precisión 0.3660\n",
            "Epoch 2 Lote 50 Pérdida 1.7098 Precisión 0.3701\n",
            "Epoch 2 Lote 100 Pérdida 1.7051 Precisión 0.3714\n",
            "Epoch 2 Lote 150 Pérdida 1.6910 Precisión 0.3723\n",
            "Epoch 2 Lote 200 Pérdida 1.6840 Precisión 0.3730\n",
            "Epoch 2 Lote 250 Pérdida 1.6751 Precisión 0.3736\n",
            "Epoch 2 Lote 300 Pérdida 1.6703 Precisión 0.3742\n",
            "Epoch 2 Lote 350 Pérdida 1.6614 Precisión 0.3752\n",
            "Epoch 2 Lote 400 Pérdida 1.6598 Precisión 0.3753\n",
            "Epoch 2 Lote 450 Pérdida 1.6600 Precisión 0.3753\n",
            "Epoch 2 Lote 500 Pérdida 1.6546 Precisión 0.3755\n",
            "Epoch 2 Lote 550 Pérdida 1.6527 Precisión 0.3757\n",
            "Epoch 2 Lote 600 Pérdida 1.6495 Precisión 0.3760\n",
            "Epoch 2 Lote 650 Pérdida 1.6469 Precisión 0.3759\n",
            "Epoch 2 Lote 700 Pérdida 1.6414 Precisión 0.3760\n",
            "Epoch 2 Lote 750 Pérdida 1.6359 Precisión 0.3773\n",
            "Epoch 2 Lote 800 Pérdida 1.6291 Precisión 0.3784\n",
            "Epoch 2 Lote 850 Pérdida 1.6239 Precisión 0.3798\n",
            "Epoch 2 Lote 900 Pérdida 1.6168 Precisión 0.3809\n",
            "Epoch 2 Lote 950 Pérdida 1.6089 Precisión 0.3824\n",
            "Epoch 2 Lote 1000 Pérdida 1.6021 Precisión 0.3839\n",
            "Epoch 2 Lote 1050 Pérdida 1.5962 Precisión 0.3848\n",
            "Epoch 2 Lote 1100 Pérdida 1.5898 Precisión 0.3855\n",
            "Epoch 2 Lote 1150 Pérdida 1.5835 Precisión 0.3866\n",
            "Epoch 2 Lote 1200 Pérdida 1.5770 Precisión 0.3876\n",
            "Epoch 2 Lote 1250 Pérdida 1.5699 Precisión 0.3884\n",
            "Epoch 2 Lote 1300 Pérdida 1.5621 Precisión 0.3893\n",
            "Epoch 2 Lote 1350 Pérdida 1.5550 Precisión 0.3901\n",
            "Epoch 2 Lote 1400 Pérdida 1.5477 Precisión 0.3910\n",
            "Epoch 2 Lote 1450 Pérdida 1.5403 Precisión 0.3918\n",
            "Epoch 2 Lote 1500 Pérdida 1.5338 Precisión 0.3927\n",
            "Epoch 2 Lote 1550 Pérdida 1.5281 Precisión 0.3934\n",
            "Epoch 2 Lote 1600 Pérdida 1.5212 Precisión 0.3943\n",
            "Epoch 2 Lote 1650 Pérdida 1.5148 Precisión 0.3952\n",
            "Epoch 2 Lote 1700 Pérdida 1.5089 Precisión 0.3960\n",
            "Epoch 2 Lote 1750 Pérdida 1.5025 Precisión 0.3966\n",
            "Epoch 2 Lote 1800 Pérdida 1.4967 Precisión 0.3973\n",
            "Epoch 2 Lote 1850 Pérdida 1.4911 Precisión 0.3978\n",
            "Epoch 2 Lote 1900 Pérdida 1.4856 Precisión 0.3984\n",
            "Epoch 2 Lote 1950 Pérdida 1.4799 Precisión 0.3989\n",
            "Epoch 2 Lote 2000 Pérdida 1.4753 Precisión 0.3995\n",
            "Epoch 2 Lote 2050 Pérdida 1.4698 Precisión 0.4002\n",
            "Epoch 2 Lote 2100 Pérdida 1.4645 Precisión 0.4007\n",
            "Epoch 2 Lote 2150 Pérdida 1.4594 Precisión 0.4013\n",
            "Epoch 2 Lote 2200 Pérdida 1.4545 Precisión 0.4018\n",
            "Epoch 2 Lote 2250 Pérdida 1.4496 Precisión 0.4023\n",
            "Epoch 2 Lote 2300 Pérdida 1.4447 Precisión 0.4028\n",
            "Epoch 2 Lote 2350 Pérdida 1.4406 Precisión 0.4034\n",
            "Epoch 2 Lote 2400 Pérdida 1.4357 Precisión 0.4039\n",
            "Epoch 2 Lote 2450 Pérdida 1.4304 Precisión 0.4045\n",
            "Epoch 2 Lote 2500 Pérdida 1.4253 Precisión 0.4051\n",
            "Epoch 2 Lote 2550 Pérdida 1.4203 Precisión 0.4057\n",
            "Epoch 2 Lote 2600 Pérdida 1.4156 Precisión 0.4063\n",
            "Epoch 2 Lote 2650 Pérdida 1.4114 Precisión 0.4068\n",
            "Epoch 2 Lote 2700 Pérdida 1.4075 Precisión 0.4074\n",
            "Epoch 2 Lote 2750 Pérdida 1.4035 Precisión 0.4080\n",
            "Epoch 2 Lote 2800 Pérdida 1.4000 Precisión 0.4084\n",
            "Epoch 2 Lote 2850 Pérdida 1.3959 Precisión 0.4088\n",
            "Epoch 2 Lote 2900 Pérdida 1.3923 Precisión 0.4093\n",
            "Epoch 2 Lote 2950 Pérdida 1.3889 Precisión 0.4098\n",
            "Epoch 2 Lote 3000 Pérdida 1.3859 Precisión 0.4102\n",
            "Epoch 2 Lote 3050 Pérdida 1.3825 Precisión 0.4107\n",
            "Epoch 2 Lote 3100 Pérdida 1.3798 Precisión 0.4112\n",
            "Epoch 2 Lote 3150 Pérdida 1.3770 Precisión 0.4118\n",
            "Epoch 2 Lote 3200 Pérdida 1.3742 Precisión 0.4123\n",
            "Epoch 2 Lote 3250 Pérdida 1.3711 Precisión 0.4129\n",
            "Epoch 2 Lote 3300 Pérdida 1.3683 Precisión 0.4135\n",
            "Epoch 2 Lote 3350 Pérdida 1.3660 Precisión 0.4140\n",
            "Epoch 2 Lote 3400 Pérdida 1.3635 Precisión 0.4144\n",
            "Epoch 2 Lote 3450 Pérdida 1.3610 Precisión 0.4149\n",
            "Epoch 2 Lote 3500 Pérdida 1.3585 Precisión 0.4155\n",
            "Epoch 2 Lote 3550 Pérdida 1.3559 Precisión 0.4160\n",
            "Epoch 2 Lote 3600 Pérdida 1.3534 Precisión 0.4165\n",
            "Epoch 2 Lote 3650 Pérdida 1.3505 Precisión 0.4171\n",
            "Epoch 2 Lote 3700 Pérdida 1.3477 Precisión 0.4176\n",
            "Epoch 2 Lote 3750 Pérdida 1.3448 Precisión 0.4181\n",
            "Epoch 2 Lote 3800 Pérdida 1.3421 Precisión 0.4186\n",
            "Epoch 2 Lote 3850 Pérdida 1.3393 Precisión 0.4191\n",
            "Epoch 2 Lote 3900 Pérdida 1.3372 Precisión 0.4196\n",
            "Epoch 2 Lote 3950 Pérdida 1.3345 Precisión 0.4201\n",
            "Epoch 2 Lote 4000 Pérdida 1.3321 Precisión 0.4206\n",
            "Epoch 2 Lote 4050 Pérdida 1.3295 Precisión 0.4212\n",
            "Epoch 2 Lote 4100 Pérdida 1.3267 Precisión 0.4217\n",
            "Epoch 2 Lote 4150 Pérdida 1.3240 Precisión 0.4223\n",
            "Epoch 2 Lote 4200 Pérdida 1.3212 Precisión 0.4229\n",
            "Epoch 2 Lote 4250 Pérdida 1.3186 Precisión 0.4234\n",
            "Epoch 2 Lote 4300 Pérdida 1.3161 Precisión 0.4240\n",
            "Epoch 2 Lote 4350 Pérdida 1.3135 Precisión 0.4245\n",
            "Epoch 2 Lote 4400 Pérdida 1.3108 Precisión 0.4251\n",
            "Epoch 2 Lote 4450 Pérdida 1.3083 Precisión 0.4257\n",
            "Epoch 2 Lote 4500 Pérdida 1.3060 Precisión 0.4263\n",
            "Epoch 2 Lote 4550 Pérdida 1.3035 Precisión 0.4268\n",
            "Epoch 2 Lote 4600 Pérdida 1.3014 Precisión 0.4273\n",
            "Epoch 2 Lote 4650 Pérdida 1.2989 Precisión 0.4278\n",
            "Epoch 2 Lote 4700 Pérdida 1.2973 Precisión 0.4282\n",
            "Epoch 2 Lote 4750 Pérdida 1.2966 Precisión 0.4284\n",
            "Epoch 2 Lote 4800 Pérdida 1.2960 Precisión 0.4286\n",
            "Epoch 2 Lote 4850 Pérdida 1.2962 Precisión 0.4287\n",
            "Epoch 2 Lote 4900 Pérdida 1.2965 Precisión 0.4287\n",
            "Epoch 2 Lote 4950 Pérdida 1.2973 Precisión 0.4286\n",
            "Epoch 2 Lote 5000 Pérdida 1.2980 Precisión 0.4286\n",
            "Epoch 2 Lote 5050 Pérdida 1.2988 Precisión 0.4285\n",
            "Epoch 2 Lote 5100 Pérdida 1.3000 Precisión 0.4284\n",
            "Epoch 2 Lote 5150 Pérdida 1.3011 Precisión 0.4284\n",
            "Epoch 2 Lote 5200 Pérdida 1.3026 Precisión 0.4282\n",
            "Epoch 2 Lote 5250 Pérdida 1.3038 Precisión 0.4280\n",
            "Epoch 2 Lote 5300 Pérdida 1.3048 Precisión 0.4279\n",
            "Epoch 2 Lote 5350 Pérdida 1.3063 Precisión 0.4277\n",
            "Epoch 2 Lote 5400 Pérdida 1.3076 Precisión 0.4277\n",
            "Epoch 2 Lote 5450 Pérdida 1.3088 Precisión 0.4275\n",
            "Epoch 2 Lote 5500 Pérdida 1.3097 Precisión 0.4274\n",
            "Epoch 2 Lote 5550 Pérdida 1.3107 Precisión 0.4273\n",
            "Epoch 2 Lote 5600 Pérdida 1.3119 Precisión 0.4271\n",
            "Epoch 2 Lote 5650 Pérdida 1.3130 Precisión 0.4270\n",
            "Epoch 2 Lote 5700 Pérdida 1.3141 Precisión 0.4268\n",
            "Epoch 2 Lote 5750 Pérdida 1.3152 Precisión 0.4267\n",
            "Epoch 2 Lote 5800 Pérdida 1.3164 Precisión 0.4265\n",
            "Epoch 2 Lote 5850 Pérdida 1.3173 Precisión 0.4264\n",
            "Epoch 2 Lote 5900 Pérdida 1.3183 Precisión 0.4262\n",
            "Epoch 2 Lote 5950 Pérdida 1.3193 Precisión 0.4260\n",
            "Epoch 2 Lote 6000 Pérdida 1.3201 Precisión 0.4258\n",
            "Epoch 2 Lote 6050 Pérdida 1.3208 Precisión 0.4256\n",
            "Epoch 2 Lote 6100 Pérdida 1.3217 Precisión 0.4254\n",
            "Epoch 2 Lote 6150 Pérdida 1.3222 Precisión 0.4253\n",
            "Epoch 2 Lote 6200 Pérdida 1.3227 Precisión 0.4251\n",
            "Epoch 2 Lote 6250 Pérdida 1.3235 Precisión 0.4249\n",
            "Epoch 2 Lote 6300 Pérdida 1.3240 Precisión 0.4248\n",
            "Epoch 2 Lote 6350 Pérdida 1.3245 Precisión 0.4247\n",
            "Epoch 2 Lote 6400 Pérdida 1.3250 Precisión 0.4246\n",
            "Guardando checkpoint para el epoch 2 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-2\n",
            "Tiempo que ha tardado 1 epoch: 1566.3543708324432 segs\n",
            "\n",
            "Inicio del epoch 3\n",
            "Epoch 3 Lote 0 Pérdida 1.6048 Precisión 0.3882\n",
            "Epoch 3 Lote 50 Pérdida 1.3913 Precisión 0.4109\n",
            "Epoch 3 Lote 100 Pérdida 1.3957 Precisión 0.4129\n",
            "Epoch 3 Lote 150 Pérdida 1.3935 Precisión 0.4118\n",
            "Epoch 3 Lote 200 Pérdida 1.3940 Precisión 0.4134\n",
            "Epoch 3 Lote 250 Pérdida 1.3900 Precisión 0.4133\n",
            "Epoch 3 Lote 300 Pérdida 1.3877 Precisión 0.4142\n",
            "Epoch 3 Lote 350 Pérdida 1.3832 Precisión 0.4146\n",
            "Epoch 3 Lote 400 Pérdida 1.3779 Precisión 0.4149\n",
            "Epoch 3 Lote 450 Pérdida 1.3767 Precisión 0.4147\n",
            "Epoch 3 Lote 500 Pérdida 1.3732 Precisión 0.4154\n",
            "Epoch 3 Lote 550 Pérdida 1.3686 Precisión 0.4155\n",
            "Epoch 3 Lote 600 Pérdida 1.3685 Precisión 0.4151\n",
            "Epoch 3 Lote 650 Pérdida 1.3670 Precisión 0.4152\n",
            "Epoch 3 Lote 700 Pérdida 1.3643 Precisión 0.4152\n",
            "Epoch 3 Lote 750 Pérdida 1.3610 Precisión 0.4161\n",
            "Epoch 3 Lote 800 Pérdida 1.3571 Precisión 0.4169\n",
            "Epoch 3 Lote 850 Pérdida 1.3522 Precisión 0.4181\n",
            "Epoch 3 Lote 900 Pérdida 1.3448 Precisión 0.4193\n",
            "Epoch 3 Lote 950 Pérdida 1.3386 Precisión 0.4206\n",
            "Epoch 3 Lote 1000 Pérdida 1.3315 Precisión 0.4215\n",
            "Epoch 3 Lote 1050 Pérdida 1.3248 Precisión 0.4227\n",
            "Epoch 3 Lote 1100 Pérdida 1.3169 Precisión 0.4235\n",
            "Epoch 3 Lote 1150 Pérdida 1.3110 Precisión 0.4242\n",
            "Epoch 3 Lote 1200 Pérdida 1.3049 Precisión 0.4252\n",
            "Epoch 3 Lote 1250 Pérdida 1.2990 Precisión 0.4259\n",
            "Epoch 3 Lote 1300 Pérdida 1.2928 Precisión 0.4265\n",
            "Epoch 3 Lote 1350 Pérdida 1.2868 Precisión 0.4273\n",
            "Epoch 3 Lote 1400 Pérdida 1.2804 Precisión 0.4281\n",
            "Epoch 3 Lote 1450 Pérdida 1.2748 Precisión 0.4291\n",
            "Epoch 3 Lote 1500 Pérdida 1.2695 Precisión 0.4298\n",
            "Epoch 3 Lote 1550 Pérdida 1.2639 Precisión 0.4306\n",
            "Epoch 3 Lote 1600 Pérdida 1.2589 Precisión 0.4314\n",
            "Epoch 3 Lote 1650 Pérdida 1.2545 Precisión 0.4322\n",
            "Epoch 3 Lote 1700 Pérdida 1.2497 Precisión 0.4330\n",
            "Epoch 3 Lote 1750 Pérdida 1.2447 Precisión 0.4336\n",
            "Epoch 3 Lote 1800 Pérdida 1.2400 Precisión 0.4342\n",
            "Epoch 3 Lote 1850 Pérdida 1.2357 Precisión 0.4346\n",
            "Epoch 3 Lote 1900 Pérdida 1.2311 Precisión 0.4350\n",
            "Epoch 3 Lote 1950 Pérdida 1.2268 Precisión 0.4354\n",
            "Epoch 3 Lote 2000 Pérdida 1.2229 Precisión 0.4358\n",
            "Epoch 3 Lote 2050 Pérdida 1.2190 Precisión 0.4363\n",
            "Epoch 3 Lote 2100 Pérdida 1.2148 Precisión 0.4368\n",
            "Epoch 3 Lote 2150 Pérdida 1.2109 Precisión 0.4372\n",
            "Epoch 3 Lote 2200 Pérdida 1.2074 Precisión 0.4377\n",
            "Epoch 3 Lote 2250 Pérdida 1.2027 Precisión 0.4381\n",
            "Epoch 3 Lote 2300 Pérdida 1.1992 Precisión 0.4385\n",
            "Epoch 3 Lote 2350 Pérdida 1.1954 Precisión 0.4389\n",
            "Epoch 3 Lote 2400 Pérdida 1.1919 Precisión 0.4394\n",
            "Epoch 3 Lote 2450 Pérdida 1.1880 Precisión 0.4399\n",
            "Epoch 3 Lote 2500 Pérdida 1.1840 Precisión 0.4403\n",
            "Epoch 3 Lote 2550 Pérdida 1.1798 Precisión 0.4407\n",
            "Epoch 3 Lote 2600 Pérdida 1.1756 Precisión 0.4411\n",
            "Epoch 3 Lote 2650 Pérdida 1.1728 Precisión 0.4416\n",
            "Epoch 3 Lote 2700 Pérdida 1.1696 Precisión 0.4420\n",
            "Epoch 3 Lote 2750 Pérdida 1.1667 Precisión 0.4423\n",
            "Epoch 3 Lote 2800 Pérdida 1.1645 Precisión 0.4426\n",
            "Epoch 3 Lote 2850 Pérdida 1.1619 Precisión 0.4429\n",
            "Epoch 3 Lote 2900 Pérdida 1.1597 Precisión 0.4433\n",
            "Epoch 3 Lote 2950 Pérdida 1.1572 Precisión 0.4437\n",
            "Epoch 3 Lote 3000 Pérdida 1.1547 Precisión 0.4442\n",
            "Epoch 3 Lote 3050 Pérdida 1.1527 Precisión 0.4446\n",
            "Epoch 3 Lote 3100 Pérdida 1.1511 Precisión 0.4450\n",
            "Epoch 3 Lote 3150 Pérdida 1.1489 Precisión 0.4454\n",
            "Epoch 3 Lote 3200 Pérdida 1.1476 Precisión 0.4458\n",
            "Epoch 3 Lote 3250 Pérdida 1.1457 Precisión 0.4462\n",
            "Epoch 3 Lote 3300 Pérdida 1.1439 Precisión 0.4466\n",
            "Epoch 3 Lote 3350 Pérdida 1.1423 Precisión 0.4469\n",
            "Epoch 3 Lote 3400 Pérdida 1.1406 Precisión 0.4473\n",
            "Epoch 3 Lote 3450 Pérdida 1.1386 Precisión 0.4477\n",
            "Epoch 3 Lote 3500 Pérdida 1.1370 Precisión 0.4481\n",
            "Epoch 3 Lote 3550 Pérdida 1.1355 Precisión 0.4485\n",
            "Epoch 3 Lote 3600 Pérdida 1.1335 Precisión 0.4489\n",
            "Epoch 3 Lote 3650 Pérdida 1.1319 Precisión 0.4493\n",
            "Epoch 3 Lote 3700 Pérdida 1.1301 Precisión 0.4496\n",
            "Epoch 3 Lote 3750 Pérdida 1.1281 Precisión 0.4500\n",
            "Epoch 3 Lote 3800 Pérdida 1.1264 Precisión 0.4504\n",
            "Epoch 3 Lote 3850 Pérdida 1.1244 Precisión 0.4508\n",
            "Epoch 3 Lote 3900 Pérdida 1.1228 Precisión 0.4512\n",
            "Epoch 3 Lote 3950 Pérdida 1.1210 Precisión 0.4515\n",
            "Epoch 3 Lote 4000 Pérdida 1.1197 Precisión 0.4519\n",
            "Epoch 3 Lote 4050 Pérdida 1.1178 Precisión 0.4523\n",
            "Epoch 3 Lote 4100 Pérdida 1.1161 Precisión 0.4527\n",
            "Epoch 3 Lote 4150 Pérdida 1.1143 Precisión 0.4531\n",
            "Epoch 3 Lote 4200 Pérdida 1.1124 Precisión 0.4536\n",
            "Epoch 3 Lote 4250 Pérdida 1.1106 Precisión 0.4540\n",
            "Epoch 3 Lote 4300 Pérdida 1.1085 Precisión 0.4545\n",
            "Epoch 3 Lote 4350 Pérdida 1.1068 Precisión 0.4550\n",
            "Epoch 3 Lote 4400 Pérdida 1.1054 Precisión 0.4555\n",
            "Epoch 3 Lote 4450 Pérdida 1.1034 Precisión 0.4559\n",
            "Epoch 3 Lote 4500 Pérdida 1.1019 Precisión 0.4564\n",
            "Epoch 3 Lote 4550 Pérdida 1.1001 Precisión 0.4569\n",
            "Epoch 3 Lote 4600 Pérdida 1.0985 Precisión 0.4574\n",
            "Epoch 3 Lote 4650 Pérdida 1.0971 Precisión 0.4578\n",
            "Epoch 3 Lote 4700 Pérdida 1.0967 Precisión 0.4581\n",
            "Epoch 3 Lote 4750 Pérdida 1.0966 Precisión 0.4582\n",
            "Epoch 3 Lote 4800 Pérdida 1.0972 Precisión 0.4582\n",
            "Epoch 3 Lote 4850 Pérdida 1.0981 Precisión 0.4582\n",
            "Epoch 3 Lote 4900 Pérdida 1.0989 Precisión 0.4580\n",
            "Epoch 3 Lote 4950 Pérdida 1.1002 Precisión 0.4579\n",
            "Epoch 3 Lote 5000 Pérdida 1.1017 Precisión 0.4577\n",
            "Epoch 3 Lote 5050 Pérdida 1.1033 Precisión 0.4576\n",
            "Epoch 3 Lote 5100 Pérdida 1.1050 Precisión 0.4574\n",
            "Epoch 3 Lote 5150 Pérdida 1.1069 Precisión 0.4571\n",
            "Epoch 3 Lote 5200 Pérdida 1.1087 Precisión 0.4569\n",
            "Epoch 3 Lote 5250 Pérdida 1.1103 Precisión 0.4567\n",
            "Epoch 3 Lote 5300 Pérdida 1.1123 Precisión 0.4564\n",
            "Epoch 3 Lote 5350 Pérdida 1.1141 Precisión 0.4562\n",
            "Epoch 3 Lote 5400 Pérdida 1.1160 Precisión 0.4560\n",
            "Epoch 3 Lote 5450 Pérdida 1.1180 Precisión 0.4557\n",
            "Epoch 3 Lote 5500 Pérdida 1.1197 Precisión 0.4555\n",
            "Epoch 3 Lote 5550 Pérdida 1.1214 Precisión 0.4553\n",
            "Epoch 3 Lote 5600 Pérdida 1.1231 Precisión 0.4550\n",
            "Epoch 3 Lote 5650 Pérdida 1.1248 Precisión 0.4548\n",
            "Epoch 3 Lote 5700 Pérdida 1.1266 Precisión 0.4546\n",
            "Epoch 3 Lote 5750 Pérdida 1.1285 Precisión 0.4543\n",
            "Epoch 3 Lote 5800 Pérdida 1.1301 Precisión 0.4541\n",
            "Epoch 3 Lote 5850 Pérdida 1.1319 Precisión 0.4538\n",
            "Epoch 3 Lote 5900 Pérdida 1.1333 Precisión 0.4535\n",
            "Epoch 3 Lote 5950 Pérdida 1.1349 Precisión 0.4532\n",
            "Epoch 3 Lote 6000 Pérdida 1.1363 Precisión 0.4529\n",
            "Epoch 3 Lote 6050 Pérdida 1.1376 Precisión 0.4526\n",
            "Epoch 3 Lote 6100 Pérdida 1.1389 Precisión 0.4524\n",
            "Epoch 3 Lote 6150 Pérdida 1.1403 Precisión 0.4521\n",
            "Epoch 3 Lote 6200 Pérdida 1.1415 Precisión 0.4519\n",
            "Epoch 3 Lote 6250 Pérdida 1.1426 Precisión 0.4516\n",
            "Epoch 3 Lote 6300 Pérdida 1.1438 Precisión 0.4514\n",
            "Epoch 3 Lote 6350 Pérdida 1.1450 Precisión 0.4512\n",
            "Epoch 3 Lote 6400 Pérdida 1.1462 Precisión 0.4510\n",
            "Guardando checkpoint para el epoch 3 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-3\n",
            "Tiempo que ha tardado 1 epoch: 1573.3830065727234 segs\n",
            "\n",
            "Inicio del epoch 4\n",
            "Epoch 4 Lote 0 Pérdida 1.2992 Precisión 0.4334\n",
            "Epoch 4 Lote 50 Pérdida 1.2857 Precisión 0.4269\n",
            "Epoch 4 Lote 100 Pérdida 1.2828 Precisión 0.4251\n",
            "Epoch 4 Lote 150 Pérdida 1.2827 Precisión 0.4247\n",
            "Epoch 4 Lote 200 Pérdida 1.2789 Precisión 0.4258\n",
            "Epoch 4 Lote 250 Pérdida 1.2760 Precisión 0.4270\n",
            "Epoch 4 Lote 300 Pérdida 1.2746 Precisión 0.4271\n",
            "Epoch 4 Lote 350 Pérdida 1.2750 Precisión 0.4284\n",
            "Epoch 4 Lote 400 Pérdida 1.2732 Precisión 0.4289\n",
            "Epoch 4 Lote 450 Pérdida 1.2719 Precisión 0.4292\n",
            "Epoch 4 Lote 500 Pérdida 1.2710 Precisión 0.4295\n",
            "Epoch 4 Lote 550 Pérdida 1.2685 Precisión 0.4298\n",
            "Epoch 4 Lote 600 Pérdida 1.2669 Precisión 0.4297\n",
            "Epoch 4 Lote 650 Pérdida 1.2673 Precisión 0.4296\n",
            "Epoch 4 Lote 700 Pérdida 1.2642 Precisión 0.4301\n",
            "Epoch 4 Lote 750 Pérdida 1.2608 Precisión 0.4305\n",
            "Epoch 4 Lote 800 Pérdida 1.2562 Precisión 0.4312\n",
            "Epoch 4 Lote 850 Pérdida 1.2510 Precisión 0.4323\n",
            "Epoch 4 Lote 900 Pérdida 1.2446 Precisión 0.4335\n",
            "Epoch 4 Lote 950 Pérdida 1.2382 Precisión 0.4346\n",
            "Epoch 4 Lote 1000 Pérdida 1.2323 Precisión 0.4359\n",
            "Epoch 4 Lote 1050 Pérdida 1.2264 Precisión 0.4370\n",
            "Epoch 4 Lote 1100 Pérdida 1.2207 Precisión 0.4378\n",
            "Epoch 4 Lote 1150 Pérdida 1.2141 Precisión 0.4387\n",
            "Epoch 4 Lote 1200 Pérdida 1.2071 Precisión 0.4395\n",
            "Epoch 4 Lote 1250 Pérdida 1.2015 Precisión 0.4401\n",
            "Epoch 4 Lote 1300 Pérdida 1.1962 Precisión 0.4411\n",
            "Epoch 4 Lote 1350 Pérdida 1.1909 Precisión 0.4419\n",
            "Epoch 4 Lote 1400 Pérdida 1.1848 Precisión 0.4424\n",
            "Epoch 4 Lote 1450 Pérdida 1.1791 Precisión 0.4433\n",
            "Epoch 4 Lote 1500 Pérdida 1.1739 Precisión 0.4442\n",
            "Epoch 4 Lote 1550 Pérdida 1.1690 Precisión 0.4451\n",
            "Epoch 4 Lote 1600 Pérdida 1.1628 Precisión 0.4459\n",
            "Epoch 4 Lote 1650 Pérdida 1.1573 Precisión 0.4468\n",
            "Epoch 4 Lote 1700 Pérdida 1.1531 Precisión 0.4475\n",
            "Epoch 4 Lote 1750 Pérdida 1.1480 Precisión 0.4482\n",
            "Epoch 4 Lote 1800 Pérdida 1.1431 Precisión 0.4486\n",
            "Epoch 4 Lote 1850 Pérdida 1.1388 Precisión 0.4492\n",
            "Epoch 4 Lote 1900 Pérdida 1.1349 Precisión 0.4496\n",
            "Epoch 4 Lote 1950 Pérdida 1.1304 Precisión 0.4500\n",
            "Epoch 4 Lote 2000 Pérdida 1.1263 Precisión 0.4505\n",
            "Epoch 4 Lote 2050 Pérdida 1.1226 Precisión 0.4508\n",
            "Epoch 4 Lote 2100 Pérdida 1.1193 Precisión 0.4512\n",
            "Epoch 4 Lote 2150 Pérdida 1.1153 Precisión 0.4517\n",
            "Epoch 4 Lote 2200 Pérdida 1.1117 Precisión 0.4520\n",
            "Epoch 4 Lote 2250 Pérdida 1.1082 Precisión 0.4524\n",
            "Epoch 4 Lote 2300 Pérdida 1.1050 Precisión 0.4528\n",
            "Epoch 4 Lote 2350 Pérdida 1.1015 Precisión 0.4532\n",
            "Epoch 4 Lote 2400 Pérdida 1.0977 Precisión 0.4537\n",
            "Epoch 4 Lote 2450 Pérdida 1.0940 Precisión 0.4541\n",
            "Epoch 4 Lote 2500 Pérdida 1.0906 Precisión 0.4546\n",
            "Epoch 4 Lote 2550 Pérdida 1.0872 Precisión 0.4548\n",
            "Epoch 4 Lote 2600 Pérdida 1.0836 Precisión 0.4551\n",
            "Epoch 4 Lote 2650 Pérdida 1.0802 Precisión 0.4556\n",
            "Epoch 4 Lote 2700 Pérdida 1.0776 Precisión 0.4559\n",
            "Epoch 4 Lote 2750 Pérdida 1.0751 Precisión 0.4562\n",
            "Epoch 4 Lote 2800 Pérdida 1.0726 Precisión 0.4567\n",
            "Epoch 4 Lote 2850 Pérdida 1.0703 Precisión 0.4570\n",
            "Epoch 4 Lote 2900 Pérdida 1.0679 Precisión 0.4574\n",
            "Epoch 4 Lote 2950 Pérdida 1.0655 Precisión 0.4577\n",
            "Epoch 4 Lote 3000 Pérdida 1.0636 Precisión 0.4581\n",
            "Epoch 4 Lote 3050 Pérdida 1.0620 Precisión 0.4584\n",
            "Epoch 4 Lote 3100 Pérdida 1.0600 Precisión 0.4588\n",
            "Epoch 4 Lote 3150 Pérdida 1.0585 Precisión 0.4593\n",
            "Epoch 4 Lote 3200 Pérdida 1.0569 Precisión 0.4596\n",
            "Epoch 4 Lote 3250 Pérdida 1.0551 Precisión 0.4600\n",
            "Epoch 4 Lote 3300 Pérdida 1.0532 Precisión 0.4604\n",
            "Epoch 4 Lote 3350 Pérdida 1.0520 Precisión 0.4606\n",
            "Epoch 4 Lote 3400 Pérdida 1.0506 Precisión 0.4610\n",
            "Epoch 4 Lote 3450 Pérdida 1.0490 Precisión 0.4614\n",
            "Epoch 4 Lote 3500 Pérdida 1.0474 Precisión 0.4618\n",
            "Epoch 4 Lote 3550 Pérdida 1.0461 Precisión 0.4622\n",
            "Epoch 4 Lote 3600 Pérdida 1.0446 Precisión 0.4625\n",
            "Epoch 4 Lote 3650 Pérdida 1.0431 Precisión 0.4628\n",
            "Epoch 4 Lote 3700 Pérdida 1.0418 Precisión 0.4631\n",
            "Epoch 4 Lote 3750 Pérdida 1.0401 Precisión 0.4634\n",
            "Epoch 4 Lote 3800 Pérdida 1.0386 Precisión 0.4638\n",
            "Epoch 4 Lote 3850 Pérdida 1.0372 Precisión 0.4642\n",
            "Epoch 4 Lote 3900 Pérdida 1.0355 Precisión 0.4646\n",
            "Epoch 4 Lote 3950 Pérdida 1.0340 Precisión 0.4649\n",
            "Epoch 4 Lote 4000 Pérdida 1.0328 Precisión 0.4653\n",
            "Epoch 4 Lote 4050 Pérdida 1.0310 Precisión 0.4656\n",
            "Epoch 4 Lote 4100 Pérdida 1.0297 Precisión 0.4660\n",
            "Epoch 4 Lote 4150 Pérdida 1.0278 Precisión 0.4664\n",
            "Epoch 4 Lote 4200 Pérdida 1.0261 Precisión 0.4668\n",
            "Epoch 4 Lote 4250 Pérdida 1.0244 Precisión 0.4673\n",
            "Epoch 4 Lote 4300 Pérdida 1.0227 Precisión 0.4677\n",
            "Epoch 4 Lote 4350 Pérdida 1.0215 Precisión 0.4681\n",
            "Epoch 4 Lote 4400 Pérdida 1.0201 Precisión 0.4686\n",
            "Epoch 4 Lote 4450 Pérdida 1.0185 Precisión 0.4690\n",
            "Epoch 4 Lote 4500 Pérdida 1.0170 Precisión 0.4694\n",
            "Epoch 4 Lote 4550 Pérdida 1.0156 Precisión 0.4699\n",
            "Epoch 4 Lote 4600 Pérdida 1.0145 Precisión 0.4702\n",
            "Epoch 4 Lote 4650 Pérdida 1.0131 Precisión 0.4706\n",
            "Epoch 4 Lote 4700 Pérdida 1.0125 Precisión 0.4708\n",
            "Epoch 4 Lote 4750 Pérdida 1.0127 Precisión 0.4709\n",
            "Epoch 4 Lote 4800 Pérdida 1.0135 Precisión 0.4709\n",
            "Epoch 4 Lote 4850 Pérdida 1.0143 Precisión 0.4709\n",
            "Epoch 4 Lote 4900 Pérdida 1.0157 Precisión 0.4709\n",
            "Epoch 4 Lote 4950 Pérdida 1.0169 Precisión 0.4707\n",
            "Epoch 4 Lote 5000 Pérdida 1.0187 Precisión 0.4705\n",
            "Epoch 4 Lote 5050 Pérdida 1.0205 Precisión 0.4703\n",
            "Epoch 4 Lote 5100 Pérdida 1.0226 Precisión 0.4700\n",
            "Epoch 4 Lote 5150 Pérdida 1.0248 Precisión 0.4697\n",
            "Epoch 4 Lote 5200 Pérdida 1.0268 Precisión 0.4694\n",
            "Epoch 4 Lote 5250 Pérdida 1.0289 Precisión 0.4691\n",
            "Epoch 4 Lote 5300 Pérdida 1.0310 Precisión 0.4688\n",
            "Epoch 4 Lote 5350 Pérdida 1.0333 Precisión 0.4686\n",
            "Epoch 4 Lote 5400 Pérdida 1.0354 Precisión 0.4683\n",
            "Epoch 4 Lote 5450 Pérdida 1.0374 Precisión 0.4681\n",
            "Epoch 4 Lote 5500 Pérdida 1.0391 Precisión 0.4679\n",
            "Epoch 4 Lote 5550 Pérdida 1.0407 Precisión 0.4676\n",
            "Epoch 4 Lote 5600 Pérdida 1.0424 Precisión 0.4674\n",
            "Epoch 4 Lote 5650 Pérdida 1.0444 Precisión 0.4671\n",
            "Epoch 4 Lote 5700 Pérdida 1.0461 Precisión 0.4668\n",
            "Epoch 4 Lote 5750 Pérdida 1.0480 Precisión 0.4666\n",
            "Epoch 4 Lote 5800 Pérdida 1.0499 Precisión 0.4663\n",
            "Epoch 4 Lote 5850 Pérdida 1.0518 Precisión 0.4660\n",
            "Epoch 4 Lote 5900 Pérdida 1.0536 Precisión 0.4657\n",
            "Epoch 4 Lote 5950 Pérdida 1.0550 Precisión 0.4654\n",
            "Epoch 4 Lote 6000 Pérdida 1.0567 Precisión 0.4651\n",
            "Epoch 4 Lote 6050 Pérdida 1.0584 Precisión 0.4648\n",
            "Epoch 4 Lote 6100 Pérdida 1.0598 Precisión 0.4645\n",
            "Epoch 4 Lote 6150 Pérdida 1.0614 Precisión 0.4642\n",
            "Epoch 4 Lote 6200 Pérdida 1.0628 Precisión 0.4639\n",
            "Epoch 4 Lote 6250 Pérdida 1.0644 Precisión 0.4637\n",
            "Epoch 4 Lote 6300 Pérdida 1.0659 Precisión 0.4634\n",
            "Epoch 4 Lote 6350 Pérdida 1.0671 Precisión 0.4632\n",
            "Epoch 4 Lote 6400 Pérdida 1.0685 Precisión 0.4629\n",
            "Guardando checkpoint para el epoch 4 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-4\n",
            "Tiempo que ha tardado 1 epoch: 1561.301760673523 segs\n",
            "\n",
            "Inicio del epoch 5\n",
            "Epoch 5 Lote 0 Pérdida 1.3921 Precisión 0.3849\n",
            "Epoch 5 Lote 50 Pérdida 1.2153 Precisión 0.4345\n",
            "Epoch 5 Lote 100 Pérdida 1.2133 Precisión 0.4358\n",
            "Epoch 5 Lote 150 Pérdida 1.2188 Precisión 0.4357\n",
            "Epoch 5 Lote 200 Pérdida 1.2193 Precisión 0.4360\n",
            "Epoch 5 Lote 250 Pérdida 1.2244 Precisión 0.4362\n",
            "Epoch 5 Lote 300 Pérdida 1.2246 Precisión 0.4374\n",
            "Epoch 5 Lote 350 Pérdida 1.2235 Precisión 0.4379\n",
            "Epoch 5 Lote 400 Pérdida 1.2197 Precisión 0.4379\n",
            "Epoch 5 Lote 450 Pérdida 1.2138 Precisión 0.4383\n",
            "Epoch 5 Lote 500 Pérdida 1.2117 Precisión 0.4384\n",
            "Epoch 5 Lote 550 Pérdida 1.2102 Precisión 0.4383\n",
            "Epoch 5 Lote 600 Pérdida 1.2099 Precisión 0.4384\n",
            "Epoch 5 Lote 650 Pérdida 1.2078 Precisión 0.4381\n",
            "Epoch 5 Lote 700 Pérdida 1.2057 Precisión 0.4381\n",
            "Epoch 5 Lote 750 Pérdida 1.2026 Precisión 0.4389\n",
            "Epoch 5 Lote 800 Pérdida 1.2001 Precisión 0.4398\n",
            "Epoch 5 Lote 850 Pérdida 1.1947 Precisión 0.4408\n",
            "Epoch 5 Lote 900 Pérdida 1.1886 Precisión 0.4420\n",
            "Epoch 5 Lote 950 Pérdida 1.1802 Precisión 0.4432\n",
            "Epoch 5 Lote 1000 Pérdida 1.1740 Precisión 0.4442\n",
            "Epoch 5 Lote 1050 Pérdida 1.1683 Precisión 0.4452\n",
            "Epoch 5 Lote 1100 Pérdida 1.1617 Precisión 0.4459\n",
            "Epoch 5 Lote 1150 Pérdida 1.1560 Precisión 0.4467\n",
            "Epoch 5 Lote 1200 Pérdida 1.1507 Precisión 0.4477\n",
            "Epoch 5 Lote 1250 Pérdida 1.1448 Precisión 0.4489\n",
            "Epoch 5 Lote 1300 Pérdida 1.1381 Precisión 0.4497\n",
            "Epoch 5 Lote 1350 Pérdida 1.1323 Precisión 0.4505\n",
            "Epoch 5 Lote 1400 Pérdida 1.1260 Precisión 0.4514\n",
            "Epoch 5 Lote 1450 Pérdida 1.1210 Precisión 0.4522\n",
            "Epoch 5 Lote 1500 Pérdida 1.1168 Precisión 0.4531\n",
            "Epoch 5 Lote 1550 Pérdida 1.1125 Precisión 0.4537\n",
            "Epoch 5 Lote 1600 Pérdida 1.1077 Precisión 0.4545\n",
            "Epoch 5 Lote 1650 Pérdida 1.1025 Precisión 0.4552\n",
            "Epoch 5 Lote 1700 Pérdida 1.0975 Precisión 0.4560\n",
            "Epoch 5 Lote 1750 Pérdida 1.0931 Precisión 0.4565\n",
            "Epoch 5 Lote 1800 Pérdida 1.0886 Precisión 0.4570\n",
            "Epoch 5 Lote 1850 Pérdida 1.0842 Precisión 0.4574\n",
            "Epoch 5 Lote 1900 Pérdida 1.0801 Precisión 0.4579\n",
            "Epoch 5 Lote 1950 Pérdida 1.0767 Precisión 0.4583\n",
            "Epoch 5 Lote 2000 Pérdida 1.0729 Precisión 0.4586\n",
            "Epoch 5 Lote 2050 Pérdida 1.0687 Precisión 0.4591\n",
            "Epoch 5 Lote 2100 Pérdida 1.0650 Precisión 0.4596\n",
            "Epoch 5 Lote 2150 Pérdida 1.0609 Precisión 0.4598\n",
            "Epoch 5 Lote 2200 Pérdida 1.0571 Precisión 0.4602\n",
            "Epoch 5 Lote 2250 Pérdida 1.0537 Precisión 0.4607\n",
            "Epoch 5 Lote 2300 Pérdida 1.0508 Precisión 0.4611\n",
            "Epoch 5 Lote 2350 Pérdida 1.0479 Precisión 0.4615\n",
            "Epoch 5 Lote 2400 Pérdida 1.0443 Precisión 0.4620\n",
            "Epoch 5 Lote 2450 Pérdida 1.0409 Precisión 0.4623\n",
            "Epoch 5 Lote 2500 Pérdida 1.0370 Precisión 0.4626\n",
            "Epoch 5 Lote 2550 Pérdida 1.0334 Precisión 0.4631\n",
            "Epoch 5 Lote 2600 Pérdida 1.0301 Precisión 0.4634\n",
            "Epoch 5 Lote 2650 Pérdida 1.0271 Precisión 0.4638\n",
            "Epoch 5 Lote 2700 Pérdida 1.0240 Precisión 0.4642\n",
            "Epoch 5 Lote 2750 Pérdida 1.0218 Precisión 0.4646\n",
            "Epoch 5 Lote 2800 Pérdida 1.0193 Precisión 0.4649\n",
            "Epoch 5 Lote 2850 Pérdida 1.0173 Precisión 0.4653\n",
            "Epoch 5 Lote 2900 Pérdida 1.0153 Precisión 0.4656\n",
            "Epoch 5 Lote 2950 Pérdida 1.0130 Precisión 0.4660\n",
            "Epoch 5 Lote 3000 Pérdida 1.0113 Precisión 0.4663\n",
            "Epoch 5 Lote 3050 Pérdida 1.0092 Precisión 0.4667\n",
            "Epoch 5 Lote 3100 Pérdida 1.0076 Precisión 0.4669\n",
            "Epoch 5 Lote 3150 Pérdida 1.0058 Precisión 0.4673\n",
            "Epoch 5 Lote 3200 Pérdida 1.0040 Precisión 0.4677\n",
            "Epoch 5 Lote 3250 Pérdida 1.0027 Precisión 0.4680\n",
            "Epoch 5 Lote 3300 Pérdida 1.0008 Precisión 0.4684\n",
            "Epoch 5 Lote 3350 Pérdida 0.9994 Precisión 0.4687\n",
            "Epoch 5 Lote 3400 Pérdida 0.9983 Precisión 0.4690\n",
            "Epoch 5 Lote 3450 Pérdida 0.9969 Precisión 0.4694\n",
            "Epoch 5 Lote 3500 Pérdida 0.9955 Precisión 0.4698\n",
            "Epoch 5 Lote 3550 Pérdida 0.9942 Precisión 0.4701\n",
            "Epoch 5 Lote 3600 Pérdida 0.9931 Precisión 0.4705\n",
            "Epoch 5 Lote 3650 Pérdida 0.9915 Precisión 0.4708\n",
            "Epoch 5 Lote 3700 Pérdida 0.9902 Precisión 0.4712\n",
            "Epoch 5 Lote 3750 Pérdida 0.9888 Precisión 0.4716\n",
            "Epoch 5 Lote 3800 Pérdida 0.9870 Precisión 0.4720\n",
            "Epoch 5 Lote 3850 Pérdida 0.9856 Precisión 0.4723\n",
            "Epoch 5 Lote 3900 Pérdida 0.9841 Precisión 0.4726\n",
            "Epoch 5 Lote 3950 Pérdida 0.9827 Precisión 0.4730\n",
            "Epoch 5 Lote 4000 Pérdida 0.9816 Precisión 0.4734\n",
            "Epoch 5 Lote 4050 Pérdida 0.9800 Precisión 0.4737\n",
            "Epoch 5 Lote 4100 Pérdida 0.9785 Precisión 0.4741\n",
            "Epoch 5 Lote 4150 Pérdida 0.9771 Precisión 0.4745\n",
            "Epoch 5 Lote 4200 Pérdida 0.9757 Precisión 0.4748\n",
            "Epoch 5 Lote 4250 Pérdida 0.9741 Precisión 0.4752\n",
            "Epoch 5 Lote 4300 Pérdida 0.9728 Precisión 0.4756\n",
            "Epoch 5 Lote 4350 Pérdida 0.9713 Precisión 0.4761\n",
            "Epoch 5 Lote 4400 Pérdida 0.9700 Precisión 0.4765\n",
            "Epoch 5 Lote 4450 Pérdida 0.9685 Precisión 0.4769\n",
            "Epoch 5 Lote 4500 Pérdida 0.9670 Precisión 0.4772\n",
            "Epoch 5 Lote 4550 Pérdida 0.9656 Precisión 0.4776\n",
            "Epoch 5 Lote 4600 Pérdida 0.9644 Precisión 0.4780\n",
            "Epoch 5 Lote 4650 Pérdida 0.9633 Precisión 0.4783\n",
            "Epoch 5 Lote 4700 Pérdida 0.9628 Precisión 0.4785\n",
            "Epoch 5 Lote 4750 Pérdida 0.9632 Precisión 0.4786\n",
            "Epoch 5 Lote 4800 Pérdida 0.9638 Precisión 0.4787\n",
            "Epoch 5 Lote 4850 Pérdida 0.9648 Precisión 0.4787\n",
            "Epoch 5 Lote 4900 Pérdida 0.9664 Precisión 0.4785\n",
            "Epoch 5 Lote 4950 Pérdida 0.9682 Precisión 0.4784\n",
            "Epoch 5 Lote 5000 Pérdida 0.9699 Precisión 0.4782\n",
            "Epoch 5 Lote 5050 Pérdida 0.9717 Precisión 0.4779\n",
            "Epoch 5 Lote 5100 Pérdida 0.9736 Precisión 0.4777\n",
            "Epoch 5 Lote 5150 Pérdida 0.9758 Precisión 0.4774\n",
            "Epoch 5 Lote 5200 Pérdida 0.9780 Precisión 0.4771\n",
            "Epoch 5 Lote 5250 Pérdida 0.9801 Precisión 0.4768\n",
            "Epoch 5 Lote 5300 Pérdida 0.9822 Precisión 0.4765\n",
            "Epoch 5 Lote 5350 Pérdida 0.9842 Precisión 0.4763\n",
            "Epoch 5 Lote 5400 Pérdida 0.9861 Precisión 0.4760\n",
            "Epoch 5 Lote 5450 Pérdida 0.9881 Precisión 0.4757\n",
            "Epoch 5 Lote 5500 Pérdida 0.9900 Precisión 0.4755\n",
            "Epoch 5 Lote 5550 Pérdida 0.9919 Precisión 0.4753\n",
            "Epoch 5 Lote 5600 Pérdida 0.9937 Precisión 0.4750\n",
            "Epoch 5 Lote 5650 Pérdida 0.9956 Precisión 0.4747\n",
            "Epoch 5 Lote 5700 Pérdida 0.9975 Precisión 0.4745\n",
            "Epoch 5 Lote 5750 Pérdida 0.9996 Precisión 0.4742\n",
            "Epoch 5 Lote 5800 Pérdida 1.0013 Precisión 0.4739\n",
            "Epoch 5 Lote 5850 Pérdida 1.0035 Precisión 0.4736\n",
            "Epoch 5 Lote 5900 Pérdida 1.0053 Precisión 0.4732\n",
            "Epoch 5 Lote 5950 Pérdida 1.0074 Precisión 0.4729\n",
            "Epoch 5 Lote 6000 Pérdida 1.0092 Precisión 0.4726\n",
            "Epoch 5 Lote 6050 Pérdida 1.0108 Precisión 0.4723\n",
            "Epoch 5 Lote 6100 Pérdida 1.0122 Precisión 0.4720\n",
            "Epoch 5 Lote 6150 Pérdida 1.0137 Precisión 0.4717\n",
            "Epoch 5 Lote 6200 Pérdida 1.0151 Precisión 0.4714\n",
            "Epoch 5 Lote 6250 Pérdida 1.0166 Precisión 0.4711\n",
            "Epoch 5 Lote 6300 Pérdida 1.0180 Precisión 0.4709\n",
            "Epoch 5 Lote 6350 Pérdida 1.0193 Precisión 0.4706\n",
            "Epoch 5 Lote 6400 Pérdida 1.0207 Precisión 0.4703\n",
            "Guardando checkpoint para el epoch 5 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-5\n",
            "Tiempo que ha tardado 1 epoch: 1557.1833729743958 segs\n",
            "\n",
            "Inicio del epoch 6\n",
            "Epoch 6 Lote 0 Pérdida 1.2198 Precisión 0.4441\n",
            "Epoch 6 Lote 50 Pérdida 1.2064 Precisión 0.4410\n",
            "Epoch 6 Lote 100 Pérdida 1.1974 Precisión 0.4463\n",
            "Epoch 6 Lote 150 Pérdida 1.1932 Precisión 0.4446\n",
            "Epoch 6 Lote 200 Pérdida 1.1833 Precisión 0.4430\n",
            "Epoch 6 Lote 250 Pérdida 1.1782 Precisión 0.4436\n",
            "Epoch 6 Lote 300 Pérdida 1.1809 Precisión 0.4436\n",
            "Epoch 6 Lote 350 Pérdida 1.1797 Precisión 0.4439\n",
            "Epoch 6 Lote 400 Pérdida 1.1765 Precisión 0.4441\n",
            "Epoch 6 Lote 450 Pérdida 1.1763 Precisión 0.4442\n",
            "Epoch 6 Lote 500 Pérdida 1.1748 Precisión 0.4446\n",
            "Epoch 6 Lote 550 Pérdida 1.1722 Precisión 0.4446\n",
            "Epoch 6 Lote 600 Pérdida 1.1711 Precisión 0.4443\n",
            "Epoch 6 Lote 650 Pérdida 1.1719 Precisión 0.4441\n",
            "Epoch 6 Lote 700 Pérdida 1.1692 Precisión 0.4448\n",
            "Epoch 6 Lote 750 Pérdida 1.1666 Precisión 0.4453\n",
            "Epoch 6 Lote 800 Pérdida 1.1627 Precisión 0.4460\n",
            "Epoch 6 Lote 850 Pérdida 1.1571 Precisión 0.4472\n",
            "Epoch 6 Lote 900 Pérdida 1.1518 Precisión 0.4484\n",
            "Epoch 6 Lote 950 Pérdida 1.1458 Precisión 0.4495\n",
            "Epoch 6 Lote 1000 Pérdida 1.1386 Precisión 0.4506\n",
            "Epoch 6 Lote 1050 Pérdida 1.1320 Precisión 0.4516\n",
            "Epoch 6 Lote 1100 Pérdida 1.1250 Precisión 0.4524\n",
            "Epoch 6 Lote 1150 Pérdida 1.1184 Precisión 0.4533\n",
            "Epoch 6 Lote 1200 Pérdida 1.1126 Precisión 0.4541\n",
            "Epoch 6 Lote 1250 Pérdida 1.1076 Precisión 0.4550\n",
            "Epoch 6 Lote 1300 Pérdida 1.1017 Precisión 0.4560\n",
            "Epoch 6 Lote 1350 Pérdida 1.0959 Precisión 0.4566\n",
            "Epoch 6 Lote 1400 Pérdida 1.0899 Precisión 0.4574\n",
            "Epoch 6 Lote 1450 Pérdida 1.0852 Precisión 0.4583\n",
            "Epoch 6 Lote 1500 Pérdida 1.0797 Precisión 0.4590\n",
            "Epoch 6 Lote 1550 Pérdida 1.0746 Precisión 0.4599\n",
            "Epoch 6 Lote 1600 Pérdida 1.0699 Precisión 0.4606\n",
            "Epoch 6 Lote 1650 Pérdida 1.0651 Precisión 0.4613\n",
            "Epoch 6 Lote 1700 Pérdida 1.0602 Precisión 0.4620\n",
            "Epoch 6 Lote 1750 Pérdida 1.0556 Precisión 0.4625\n",
            "Epoch 6 Lote 1800 Pérdida 1.0512 Precisión 0.4632\n",
            "Epoch 6 Lote 1850 Pérdida 1.0470 Precisión 0.4637\n",
            "Epoch 6 Lote 1900 Pérdida 1.0430 Precisión 0.4642\n",
            "Epoch 6 Lote 1950 Pérdida 1.0394 Precisión 0.4646\n",
            "Epoch 6 Lote 2000 Pérdida 1.0354 Precisión 0.4649\n",
            "Epoch 6 Lote 2050 Pérdida 1.0317 Precisión 0.4652\n",
            "Epoch 6 Lote 2100 Pérdida 1.0279 Precisión 0.4655\n",
            "Epoch 6 Lote 2150 Pérdida 1.0245 Precisión 0.4659\n",
            "Epoch 6 Lote 2200 Pérdida 1.0212 Precisión 0.4662\n",
            "Epoch 6 Lote 2250 Pérdida 1.0178 Precisión 0.4666\n",
            "Epoch 6 Lote 2300 Pérdida 1.0144 Precisión 0.4669\n",
            "Epoch 6 Lote 2350 Pérdida 1.0112 Precisión 0.4673\n",
            "Epoch 6 Lote 2400 Pérdida 1.0075 Precisión 0.4678\n",
            "Epoch 6 Lote 2450 Pérdida 1.0041 Precisión 0.4682\n",
            "Epoch 6 Lote 2500 Pérdida 1.0006 Precisión 0.4686\n",
            "Epoch 6 Lote 2550 Pérdida 0.9971 Precisión 0.4690\n",
            "Epoch 6 Lote 2600 Pérdida 0.9932 Precisión 0.4694\n",
            "Epoch 6 Lote 2650 Pérdida 0.9905 Precisión 0.4697\n",
            "Epoch 6 Lote 2700 Pérdida 0.9878 Precisión 0.4700\n",
            "Epoch 6 Lote 2750 Pérdida 0.9853 Precisión 0.4704\n",
            "Epoch 6 Lote 2800 Pérdida 0.9832 Precisión 0.4708\n",
            "Epoch 6 Lote 2850 Pérdida 0.9815 Precisión 0.4711\n",
            "Epoch 6 Lote 2900 Pérdida 0.9794 Precisión 0.4714\n",
            "Epoch 6 Lote 2950 Pérdida 0.9772 Precisión 0.4718\n",
            "Epoch 6 Lote 3000 Pérdida 0.9751 Precisión 0.4721\n",
            "Epoch 6 Lote 3050 Pérdida 0.9735 Precisión 0.4724\n",
            "Epoch 6 Lote 3100 Pérdida 0.9717 Precisión 0.4728\n",
            "Epoch 6 Lote 3150 Pérdida 0.9701 Precisión 0.4731\n",
            "Epoch 6 Lote 3200 Pérdida 0.9683 Precisión 0.4735\n",
            "Epoch 6 Lote 3250 Pérdida 0.9671 Precisión 0.4738\n",
            "Epoch 6 Lote 3300 Pérdida 0.9657 Precisión 0.4743\n",
            "Epoch 6 Lote 3350 Pérdida 0.9645 Precisión 0.4745\n",
            "Epoch 6 Lote 3400 Pérdida 0.9636 Precisión 0.4749\n",
            "Epoch 6 Lote 3450 Pérdida 0.9625 Precisión 0.4751\n",
            "Epoch 6 Lote 3500 Pérdida 0.9609 Precisión 0.4755\n",
            "Epoch 6 Lote 3550 Pérdida 0.9597 Precisión 0.4759\n",
            "Epoch 6 Lote 3600 Pérdida 0.9583 Precisión 0.4762\n",
            "Epoch 6 Lote 3650 Pérdida 0.9568 Precisión 0.4765\n",
            "Epoch 6 Lote 3700 Pérdida 0.9553 Precisión 0.4768\n",
            "Epoch 6 Lote 3750 Pérdida 0.9541 Precisión 0.4771\n",
            "Epoch 6 Lote 3800 Pérdida 0.9526 Precisión 0.4774\n",
            "Epoch 6 Lote 3850 Pérdida 0.9515 Precisión 0.4777\n",
            "Epoch 6 Lote 3900 Pérdida 0.9500 Precisión 0.4781\n",
            "Epoch 6 Lote 3950 Pérdida 0.9489 Precisión 0.4784\n",
            "Epoch 6 Lote 4000 Pérdida 0.9472 Precisión 0.4788\n",
            "Epoch 6 Lote 4050 Pérdida 0.9460 Precisión 0.4791\n",
            "Epoch 6 Lote 4100 Pérdida 0.9443 Precisión 0.4794\n",
            "Epoch 6 Lote 4150 Pérdida 0.9432 Precisión 0.4799\n",
            "Epoch 6 Lote 4200 Pérdida 0.9416 Precisión 0.4803\n",
            "Epoch 6 Lote 4250 Pérdida 0.9401 Precisión 0.4806\n",
            "Epoch 6 Lote 4300 Pérdida 0.9387 Precisión 0.4811\n",
            "Epoch 6 Lote 4350 Pérdida 0.9375 Precisión 0.4815\n",
            "Epoch 6 Lote 4400 Pérdida 0.9362 Precisión 0.4819\n",
            "Epoch 6 Lote 4450 Pérdida 0.9350 Precisión 0.4823\n",
            "Epoch 6 Lote 4500 Pérdida 0.9336 Precisión 0.4827\n",
            "Epoch 6 Lote 4550 Pérdida 0.9323 Precisión 0.4831\n",
            "Epoch 6 Lote 4600 Pérdida 0.9312 Precisión 0.4835\n",
            "Epoch 6 Lote 4650 Pérdida 0.9302 Precisión 0.4838\n",
            "Epoch 6 Lote 4700 Pérdida 0.9297 Precisión 0.4840\n",
            "Epoch 6 Lote 4750 Pérdida 0.9300 Precisión 0.4840\n",
            "Epoch 6 Lote 4800 Pérdida 0.9308 Precisión 0.4841\n",
            "Epoch 6 Lote 4850 Pérdida 0.9320 Precisión 0.4840\n",
            "Epoch 6 Lote 4900 Pérdida 0.9333 Precisión 0.4839\n",
            "Epoch 6 Lote 4950 Pérdida 0.9349 Precisión 0.4837\n",
            "Epoch 6 Lote 5000 Pérdida 0.9365 Precisión 0.4835\n",
            "Epoch 6 Lote 5050 Pérdida 0.9385 Precisión 0.4832\n",
            "Epoch 6 Lote 5100 Pérdida 0.9406 Precisión 0.4830\n",
            "Epoch 6 Lote 5150 Pérdida 0.9428 Precisión 0.4828\n",
            "Epoch 6 Lote 5200 Pérdida 0.9449 Precisión 0.4825\n",
            "Epoch 6 Lote 5250 Pérdida 0.9471 Precisión 0.4822\n",
            "Epoch 6 Lote 5300 Pérdida 0.9494 Precisión 0.4819\n",
            "Epoch 6 Lote 5350 Pérdida 0.9515 Precisión 0.4816\n",
            "Epoch 6 Lote 5400 Pérdida 0.9535 Precisión 0.4814\n",
            "Epoch 6 Lote 5450 Pérdida 0.9556 Precisión 0.4811\n",
            "Epoch 6 Lote 5500 Pérdida 0.9574 Precisión 0.4808\n",
            "Epoch 6 Lote 5550 Pérdida 0.9594 Precisión 0.4806\n",
            "Epoch 6 Lote 5600 Pérdida 0.9613 Precisión 0.4803\n",
            "Epoch 6 Lote 5650 Pérdida 0.9631 Precisión 0.4800\n",
            "Epoch 6 Lote 5700 Pérdida 0.9651 Precisión 0.4797\n",
            "Epoch 6 Lote 5750 Pérdida 0.9670 Precisión 0.4794\n",
            "Epoch 6 Lote 5800 Pérdida 0.9690 Precisión 0.4791\n",
            "Epoch 6 Lote 5850 Pérdida 0.9709 Precisión 0.4788\n",
            "Epoch 6 Lote 5900 Pérdida 0.9728 Precisión 0.4785\n",
            "Epoch 6 Lote 5950 Pérdida 0.9748 Precisión 0.4781\n",
            "Epoch 6 Lote 6000 Pérdida 0.9764 Precisión 0.4778\n",
            "Epoch 6 Lote 6050 Pérdida 0.9782 Precisión 0.4774\n",
            "Epoch 6 Lote 6100 Pérdida 0.9799 Precisión 0.4772\n",
            "Epoch 6 Lote 6150 Pérdida 0.9812 Precisión 0.4769\n",
            "Epoch 6 Lote 6200 Pérdida 0.9828 Precisión 0.4766\n",
            "Epoch 6 Lote 6250 Pérdida 0.9841 Precisión 0.4763\n",
            "Epoch 6 Lote 6300 Pérdida 0.9857 Precisión 0.4760\n",
            "Epoch 6 Lote 6350 Pérdida 0.9872 Precisión 0.4758\n",
            "Epoch 6 Lote 6400 Pérdida 0.9885 Precisión 0.4755\n",
            "Guardando checkpoint para el epoch 6 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-6\n",
            "Tiempo que ha tardado 1 epoch: 1584.3481879234314 segs\n",
            "\n",
            "Inicio del epoch 7\n",
            "Epoch 7 Lote 0 Pérdida 1.0520 Precisión 0.4523\n",
            "Epoch 7 Lote 50 Pérdida 1.1865 Precisión 0.4419\n",
            "Epoch 7 Lote 100 Pérdida 1.1674 Precisión 0.4457\n",
            "Epoch 7 Lote 150 Pérdida 1.1529 Precisión 0.4477\n",
            "Epoch 7 Lote 200 Pérdida 1.1563 Precisión 0.4478\n",
            "Epoch 7 Lote 250 Pérdida 1.1578 Precisión 0.4473\n",
            "Epoch 7 Lote 300 Pérdida 1.1557 Precisión 0.4485\n",
            "Epoch 7 Lote 350 Pérdida 1.1567 Precisión 0.4490\n",
            "Epoch 7 Lote 400 Pérdida 1.1558 Precisión 0.4492\n",
            "Epoch 7 Lote 450 Pérdida 1.1511 Precisión 0.4489\n",
            "Epoch 7 Lote 500 Pérdida 1.1495 Precisión 0.4488\n",
            "Epoch 7 Lote 550 Pérdida 1.1458 Precisión 0.4486\n",
            "Epoch 7 Lote 600 Pérdida 1.1415 Precisión 0.4485\n",
            "Epoch 7 Lote 650 Pérdida 1.1426 Precisión 0.4487\n",
            "Epoch 7 Lote 700 Pérdida 1.1411 Precisión 0.4486\n",
            "Epoch 7 Lote 750 Pérdida 1.1376 Precisión 0.4493\n",
            "Epoch 7 Lote 800 Pérdida 1.1338 Precisión 0.4504\n",
            "Epoch 7 Lote 850 Pérdida 1.1278 Precisión 0.4515\n",
            "Epoch 7 Lote 900 Pérdida 1.1212 Precisión 0.4527\n",
            "Epoch 7 Lote 950 Pérdida 1.1146 Precisión 0.4541\n",
            "Epoch 7 Lote 1000 Pérdida 1.1096 Precisión 0.4551\n",
            "Epoch 7 Lote 1050 Pérdida 1.1040 Precisión 0.4559\n",
            "Epoch 7 Lote 1100 Pérdida 1.0975 Precisión 0.4567\n",
            "Epoch 7 Lote 1150 Pérdida 1.0910 Precisión 0.4576\n",
            "Epoch 7 Lote 1200 Pérdida 1.0850 Precisión 0.4585\n",
            "Epoch 7 Lote 1250 Pérdida 1.0795 Precisión 0.4592\n",
            "Epoch 7 Lote 1300 Pérdida 1.0741 Precisión 0.4601\n",
            "Epoch 7 Lote 1350 Pérdida 1.0682 Precisión 0.4610\n",
            "Epoch 7 Lote 1400 Pérdida 1.0626 Precisión 0.4618\n",
            "Epoch 7 Lote 1450 Pérdida 1.0567 Precisión 0.4626\n",
            "Epoch 7 Lote 1500 Pérdida 1.0521 Precisión 0.4634\n",
            "Epoch 7 Lote 1550 Pérdida 1.0470 Precisión 0.4642\n",
            "Epoch 7 Lote 1600 Pérdida 1.0425 Precisión 0.4648\n",
            "Epoch 7 Lote 1650 Pérdida 1.0381 Precisión 0.4656\n",
            "Epoch 7 Lote 1700 Pérdida 1.0335 Precisión 0.4664\n",
            "Epoch 7 Lote 1750 Pérdida 1.0293 Precisión 0.4671\n",
            "Epoch 7 Lote 1800 Pérdida 1.0251 Precisión 0.4675\n",
            "Epoch 7 Lote 1850 Pérdida 1.0209 Precisión 0.4679\n",
            "Epoch 7 Lote 1900 Pérdida 1.0165 Precisión 0.4683\n",
            "Epoch 7 Lote 1950 Pérdida 1.0126 Precisión 0.4687\n",
            "Epoch 7 Lote 2000 Pérdida 1.0091 Precisión 0.4689\n",
            "Epoch 7 Lote 2050 Pérdida 1.0054 Precisión 0.4694\n",
            "Epoch 7 Lote 2100 Pérdida 1.0018 Precisión 0.4698\n",
            "Epoch 7 Lote 2150 Pérdida 0.9980 Precisión 0.4701\n",
            "Epoch 7 Lote 2200 Pérdida 0.9947 Precisión 0.4704\n",
            "Epoch 7 Lote 2250 Pérdida 0.9913 Precisión 0.4708\n",
            "Epoch 7 Lote 2300 Pérdida 0.9878 Precisión 0.4713\n",
            "Epoch 7 Lote 2350 Pérdida 0.9840 Precisión 0.4716\n",
            "Epoch 7 Lote 2400 Pérdida 0.9805 Precisión 0.4721\n",
            "Epoch 7 Lote 2450 Pérdida 0.9769 Precisión 0.4724\n",
            "Epoch 7 Lote 2500 Pérdida 0.9734 Precisión 0.4728\n",
            "Epoch 7 Lote 2550 Pérdida 0.9698 Precisión 0.4731\n",
            "Epoch 7 Lote 2600 Pérdida 0.9666 Precisión 0.4735\n",
            "Epoch 7 Lote 2650 Pérdida 0.9636 Precisión 0.4737\n",
            "Epoch 7 Lote 2700 Pérdida 0.9616 Precisión 0.4742\n",
            "Epoch 7 Lote 2750 Pérdida 0.9592 Precisión 0.4747\n",
            "Epoch 7 Lote 2800 Pérdida 0.9569 Precisión 0.4749\n",
            "Epoch 7 Lote 2850 Pérdida 0.9548 Precisión 0.4752\n",
            "Epoch 7 Lote 2900 Pérdida 0.9526 Precisión 0.4755\n",
            "Epoch 7 Lote 2950 Pérdida 0.9509 Precisión 0.4758\n",
            "Epoch 7 Lote 3000 Pérdida 0.9493 Precisión 0.4762\n",
            "Epoch 7 Lote 3050 Pérdida 0.9475 Precisión 0.4766\n",
            "Epoch 7 Lote 3100 Pérdida 0.9457 Precisión 0.4770\n",
            "Epoch 7 Lote 3150 Pérdida 0.9441 Precisión 0.4773\n",
            "Epoch 7 Lote 3200 Pérdida 0.9427 Precisión 0.4777\n",
            "Epoch 7 Lote 3250 Pérdida 0.9412 Precisión 0.4781\n",
            "Epoch 7 Lote 3300 Pérdida 0.9397 Precisión 0.4783\n",
            "Epoch 7 Lote 3350 Pérdida 0.9384 Precisión 0.4787\n",
            "Epoch 7 Lote 3400 Pérdida 0.9371 Precisión 0.4791\n",
            "Epoch 7 Lote 3450 Pérdida 0.9358 Precisión 0.4794\n",
            "Epoch 7 Lote 3500 Pérdida 0.9346 Precisión 0.4797\n",
            "Epoch 7 Lote 3550 Pérdida 0.9333 Precisión 0.4801\n",
            "Epoch 7 Lote 3600 Pérdida 0.9320 Precisión 0.4804\n",
            "Epoch 7 Lote 3650 Pérdida 0.9306 Precisión 0.4807\n",
            "Epoch 7 Lote 3700 Pérdida 0.9290 Precisión 0.4810\n",
            "Epoch 7 Lote 3750 Pérdida 0.9277 Precisión 0.4813\n",
            "Epoch 7 Lote 3800 Pérdida 0.9263 Precisión 0.4816\n",
            "Epoch 7 Lote 3850 Pérdida 0.9251 Precisión 0.4818\n",
            "Epoch 7 Lote 3900 Pérdida 0.9238 Precisión 0.4821\n",
            "Epoch 7 Lote 3950 Pérdida 0.9225 Precisión 0.4824\n",
            "Epoch 7 Lote 4000 Pérdida 0.9212 Precisión 0.4827\n",
            "Epoch 7 Lote 4050 Pérdida 0.9198 Precisión 0.4831\n",
            "Epoch 7 Lote 4100 Pérdida 0.9183 Precisión 0.4835\n",
            "Epoch 7 Lote 4150 Pérdida 0.9169 Precisión 0.4838\n",
            "Epoch 7 Lote 4200 Pérdida 0.9156 Precisión 0.4842\n",
            "Epoch 7 Lote 4250 Pérdida 0.9144 Precisión 0.4847\n",
            "Epoch 7 Lote 4300 Pérdida 0.9131 Precisión 0.4851\n",
            "Epoch 7 Lote 4350 Pérdida 0.9118 Precisión 0.4854\n",
            "Epoch 7 Lote 4400 Pérdida 0.9105 Precisión 0.4858\n",
            "Epoch 7 Lote 4450 Pérdida 0.9092 Precisión 0.4861\n",
            "Epoch 7 Lote 4500 Pérdida 0.9080 Precisión 0.4866\n",
            "Epoch 7 Lote 4550 Pérdida 0.9068 Precisión 0.4870\n",
            "Epoch 7 Lote 4600 Pérdida 0.9058 Precisión 0.4874\n",
            "Epoch 7 Lote 4650 Pérdida 0.9048 Precisión 0.4877\n",
            "Epoch 7 Lote 4700 Pérdida 0.9047 Precisión 0.4879\n",
            "Epoch 7 Lote 4750 Pérdida 0.9050 Precisión 0.4880\n",
            "Epoch 7 Lote 4800 Pérdida 0.9058 Precisión 0.4880\n",
            "Epoch 7 Lote 4850 Pérdida 0.9067 Precisión 0.4880\n",
            "Epoch 7 Lote 4900 Pérdida 0.9084 Precisión 0.4878\n",
            "Epoch 7 Lote 4950 Pérdida 0.9099 Precisión 0.4877\n",
            "Epoch 7 Lote 5000 Pérdida 0.9115 Precisión 0.4875\n",
            "Epoch 7 Lote 5050 Pérdida 0.9134 Precisión 0.4873\n",
            "Epoch 7 Lote 5100 Pérdida 0.9152 Precisión 0.4871\n",
            "Epoch 7 Lote 5150 Pérdida 0.9173 Precisión 0.4868\n",
            "Epoch 7 Lote 5200 Pérdida 0.9196 Precisión 0.4865\n",
            "Epoch 7 Lote 5250 Pérdida 0.9216 Precisión 0.4862\n",
            "Epoch 7 Lote 5300 Pérdida 0.9239 Precisión 0.4859\n",
            "Epoch 7 Lote 5350 Pérdida 0.9259 Precisión 0.4856\n",
            "Epoch 7 Lote 5400 Pérdida 0.9280 Precisión 0.4853\n",
            "Epoch 7 Lote 5450 Pérdida 0.9301 Precisión 0.4851\n",
            "Epoch 7 Lote 5500 Pérdida 0.9321 Precisión 0.4848\n",
            "Epoch 7 Lote 5550 Pérdida 0.9343 Precisión 0.4845\n",
            "Epoch 7 Lote 5600 Pérdida 0.9362 Precisión 0.4843\n",
            "Epoch 7 Lote 5650 Pérdida 0.9381 Precisión 0.4840\n",
            "Epoch 7 Lote 5700 Pérdida 0.9402 Precisión 0.4837\n",
            "Epoch 7 Lote 5750 Pérdida 0.9423 Precisión 0.4834\n",
            "Epoch 7 Lote 5800 Pérdida 0.9446 Precisión 0.4831\n",
            "Epoch 7 Lote 5850 Pérdida 0.9463 Precisión 0.4827\n",
            "Epoch 7 Lote 5900 Pérdida 0.9484 Precisión 0.4824\n",
            "Epoch 7 Lote 5950 Pérdida 0.9502 Precisión 0.4821\n",
            "Epoch 7 Lote 6000 Pérdida 0.9521 Precisión 0.4816\n",
            "Epoch 7 Lote 6050 Pérdida 0.9536 Precisión 0.4813\n",
            "Epoch 7 Lote 6100 Pérdida 0.9553 Precisión 0.4810\n",
            "Epoch 7 Lote 6150 Pérdida 0.9569 Precisión 0.4807\n",
            "Epoch 7 Lote 6200 Pérdida 0.9584 Precisión 0.4804\n",
            "Epoch 7 Lote 6250 Pérdida 0.9598 Precisión 0.4801\n",
            "Epoch 7 Lote 6300 Pérdida 0.9613 Precisión 0.4799\n",
            "Epoch 7 Lote 6350 Pérdida 0.9628 Precisión 0.4796\n",
            "Epoch 7 Lote 6400 Pérdida 0.9640 Precisión 0.4794\n",
            "Guardando checkpoint para el epoch 7 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-7\n",
            "Tiempo que ha tardado 1 epoch: 1587.6691868305206 segs\n",
            "\n",
            "Inicio del epoch 8\n",
            "Epoch 8 Lote 0 Pérdida 1.1441 Precisión 0.4786\n",
            "Epoch 8 Lote 50 Pérdida 1.1260 Precisión 0.4545\n",
            "Epoch 8 Lote 100 Pérdida 1.1411 Precisión 0.4527\n",
            "Epoch 8 Lote 150 Pérdida 1.1424 Precisión 0.4529\n",
            "Epoch 8 Lote 200 Pérdida 1.1385 Precisión 0.4521\n",
            "Epoch 8 Lote 250 Pérdida 1.1377 Precisión 0.4521\n",
            "Epoch 8 Lote 300 Pérdida 1.1389 Precisión 0.4525\n",
            "Epoch 8 Lote 350 Pérdida 1.1361 Precisión 0.4526\n",
            "Epoch 8 Lote 400 Pérdida 1.1288 Precisión 0.4526\n",
            "Epoch 8 Lote 450 Pérdida 1.1257 Precisión 0.4529\n",
            "Epoch 8 Lote 500 Pérdida 1.1228 Precisión 0.4530\n",
            "Epoch 8 Lote 550 Pérdida 1.1205 Precisión 0.4529\n",
            "Epoch 8 Lote 600 Pérdida 1.1193 Precisión 0.4531\n",
            "Epoch 8 Lote 650 Pérdida 1.1187 Precisión 0.4527\n",
            "Epoch 8 Lote 700 Pérdida 1.1189 Precisión 0.4529\n",
            "Epoch 8 Lote 750 Pérdida 1.1155 Precisión 0.4533\n",
            "Epoch 8 Lote 800 Pérdida 1.1118 Precisión 0.4542\n",
            "Epoch 8 Lote 850 Pérdida 1.1068 Precisión 0.4553\n",
            "Epoch 8 Lote 900 Pérdida 1.1006 Precisión 0.4566\n",
            "Epoch 8 Lote 950 Pérdida 1.0939 Precisión 0.4580\n",
            "Epoch 8 Lote 1000 Pérdida 1.0877 Precisión 0.4593\n",
            "Epoch 8 Lote 1050 Pérdida 1.0821 Precisión 0.4604\n",
            "Epoch 8 Lote 1100 Pérdida 1.0763 Precisión 0.4610\n",
            "Epoch 8 Lote 1150 Pérdida 1.0706 Precisión 0.4615\n",
            "Epoch 8 Lote 1200 Pérdida 1.0651 Precisión 0.4622\n",
            "Epoch 8 Lote 1250 Pérdida 1.0581 Precisión 0.4629\n",
            "Epoch 8 Lote 1300 Pérdida 1.0522 Precisión 0.4636\n",
            "Epoch 8 Lote 1350 Pérdida 1.0466 Precisión 0.4644\n",
            "Epoch 8 Lote 1400 Pérdida 1.0414 Precisión 0.4653\n",
            "Epoch 8 Lote 1450 Pérdida 1.0366 Precisión 0.4663\n",
            "Epoch 8 Lote 1500 Pérdida 1.0317 Precisión 0.4671\n",
            "Epoch 8 Lote 1550 Pérdida 1.0271 Precisión 0.4679\n",
            "Epoch 8 Lote 1600 Pérdida 1.0220 Precisión 0.4686\n",
            "Epoch 8 Lote 1650 Pérdida 1.0170 Precisión 0.4694\n",
            "Epoch 8 Lote 1700 Pérdida 1.0125 Precisión 0.4700\n",
            "Epoch 8 Lote 1750 Pérdida 1.0084 Precisión 0.4705\n",
            "Epoch 8 Lote 1800 Pérdida 1.0037 Precisión 0.4709\n",
            "Epoch 8 Lote 1850 Pérdida 0.9992 Precisión 0.4714\n",
            "Epoch 8 Lote 1900 Pérdida 0.9953 Precisión 0.4716\n",
            "Epoch 8 Lote 1950 Pérdida 0.9911 Precisión 0.4721\n",
            "Epoch 8 Lote 2000 Pérdida 0.9870 Precisión 0.4726\n",
            "Epoch 8 Lote 2050 Pérdida 0.9830 Precisión 0.4728\n",
            "Epoch 8 Lote 2100 Pérdida 0.9791 Precisión 0.4731\n",
            "Epoch 8 Lote 2150 Pérdida 0.9755 Precisión 0.4736\n",
            "Epoch 8 Lote 2200 Pérdida 0.9720 Precisión 0.4741\n",
            "Epoch 8 Lote 2250 Pérdida 0.9686 Precisión 0.4746\n",
            "Epoch 8 Lote 2300 Pérdida 0.9653 Precisión 0.4750\n",
            "Epoch 8 Lote 2350 Pérdida 0.9621 Precisión 0.4753\n",
            "Epoch 8 Lote 2400 Pérdida 0.9588 Precisión 0.4757\n",
            "Epoch 8 Lote 2450 Pérdida 0.9555 Precisión 0.4761\n",
            "Epoch 8 Lote 2500 Pérdida 0.9520 Precisión 0.4765\n",
            "Epoch 8 Lote 2550 Pérdida 0.9483 Precisión 0.4769\n",
            "Epoch 8 Lote 2600 Pérdida 0.9448 Precisión 0.4773\n",
            "Epoch 8 Lote 2650 Pérdida 0.9423 Precisión 0.4776\n",
            "Epoch 8 Lote 2700 Pérdida 0.9396 Precisión 0.4779\n",
            "Epoch 8 Lote 2750 Pérdida 0.9375 Precisión 0.4782\n",
            "Epoch 8 Lote 2800 Pérdida 0.9350 Precisión 0.4784\n",
            "Epoch 8 Lote 2850 Pérdida 0.9332 Precisión 0.4787\n",
            "Epoch 8 Lote 2900 Pérdida 0.9313 Precisión 0.4790\n",
            "Epoch 8 Lote 2950 Pérdida 0.9294 Precisión 0.4794\n",
            "Epoch 8 Lote 3000 Pérdida 0.9278 Precisión 0.4797\n",
            "Epoch 8 Lote 3050 Pérdida 0.9262 Precisión 0.4800\n",
            "Epoch 8 Lote 3100 Pérdida 0.9244 Precisión 0.4804\n",
            "Epoch 8 Lote 3150 Pérdida 0.9229 Precisión 0.4808\n",
            "Epoch 8 Lote 3200 Pérdida 0.9216 Precisión 0.4812\n",
            "Epoch 8 Lote 3250 Pérdida 0.9200 Precisión 0.4815\n",
            "Epoch 8 Lote 3300 Pérdida 0.9187 Precisión 0.4819\n",
            "Epoch 8 Lote 3350 Pérdida 0.9176 Precisión 0.4821\n",
            "Epoch 8 Lote 3400 Pérdida 0.9161 Precisión 0.4824\n",
            "Epoch 8 Lote 3450 Pérdida 0.9148 Precisión 0.4828\n",
            "Epoch 8 Lote 3500 Pérdida 0.9139 Precisión 0.4831\n",
            "Epoch 8 Lote 3550 Pérdida 0.9127 Precisión 0.4835\n",
            "Epoch 8 Lote 3600 Pérdida 0.9111 Precisión 0.4838\n",
            "Epoch 8 Lote 3650 Pérdida 0.9100 Precisión 0.4841\n",
            "Epoch 8 Lote 3700 Pérdida 0.9086 Precisión 0.4844\n",
            "Epoch 8 Lote 3750 Pérdida 0.9075 Precisión 0.4847\n",
            "Epoch 8 Lote 3800 Pérdida 0.9062 Precisión 0.4850\n",
            "Epoch 8 Lote 3850 Pérdida 0.9050 Precisión 0.4853\n",
            "Epoch 8 Lote 3900 Pérdida 0.9039 Precisión 0.4856\n",
            "Epoch 8 Lote 3950 Pérdida 0.9025 Precisión 0.4859\n",
            "Epoch 8 Lote 4000 Pérdida 0.9011 Precisión 0.4863\n",
            "Epoch 8 Lote 4050 Pérdida 0.8999 Precisión 0.4867\n",
            "Epoch 8 Lote 4100 Pérdida 0.8985 Precisión 0.4870\n",
            "Epoch 8 Lote 4150 Pérdida 0.8970 Precisión 0.4875\n",
            "Epoch 8 Lote 4200 Pérdida 0.8956 Precisión 0.4879\n",
            "Epoch 8 Lote 4250 Pérdida 0.8941 Precisión 0.4883\n",
            "Epoch 8 Lote 4300 Pérdida 0.8929 Precisión 0.4887\n",
            "Epoch 8 Lote 4350 Pérdida 0.8917 Precisión 0.4891\n",
            "Epoch 8 Lote 4400 Pérdida 0.8905 Precisión 0.4894\n",
            "Epoch 8 Lote 4450 Pérdida 0.8890 Precisión 0.4898\n",
            "Epoch 8 Lote 4500 Pérdida 0.8878 Precisión 0.4902\n",
            "Epoch 8 Lote 4550 Pérdida 0.8867 Precisión 0.4905\n",
            "Epoch 8 Lote 4600 Pérdida 0.8856 Precisión 0.4909\n",
            "Epoch 8 Lote 4650 Pérdida 0.8848 Precisión 0.4912\n",
            "Epoch 8 Lote 4700 Pérdida 0.8844 Precisión 0.4914\n",
            "Epoch 8 Lote 4750 Pérdida 0.8846 Precisión 0.4914\n",
            "Epoch 8 Lote 4800 Pérdida 0.8852 Precisión 0.4914\n",
            "Epoch 8 Lote 4850 Pérdida 0.8862 Precisión 0.4914\n",
            "Epoch 8 Lote 4900 Pérdida 0.8877 Precisión 0.4913\n",
            "Epoch 8 Lote 4950 Pérdida 0.8893 Precisión 0.4911\n",
            "Epoch 8 Lote 5000 Pérdida 0.8912 Precisión 0.4908\n",
            "Epoch 8 Lote 5050 Pérdida 0.8931 Precisión 0.4906\n",
            "Epoch 8 Lote 5100 Pérdida 0.8951 Precisión 0.4903\n",
            "Epoch 8 Lote 5150 Pérdida 0.8972 Precisión 0.4901\n",
            "Epoch 8 Lote 5200 Pérdida 0.8992 Precisión 0.4898\n",
            "Epoch 8 Lote 5250 Pérdida 0.9014 Precisión 0.4895\n",
            "Epoch 8 Lote 5300 Pérdida 0.9035 Precisión 0.4892\n",
            "Epoch 8 Lote 5350 Pérdida 0.9058 Precisión 0.4889\n",
            "Epoch 8 Lote 5400 Pérdida 0.9079 Precisión 0.4886\n",
            "Epoch 8 Lote 5450 Pérdida 0.9101 Precisión 0.4884\n",
            "Epoch 8 Lote 5500 Pérdida 0.9122 Precisión 0.4881\n",
            "Epoch 8 Lote 5550 Pérdida 0.9141 Precisión 0.4878\n",
            "Epoch 8 Lote 5600 Pérdida 0.9160 Precisión 0.4875\n",
            "Epoch 8 Lote 5650 Pérdida 0.9178 Precisión 0.4872\n",
            "Epoch 8 Lote 5700 Pérdida 0.9199 Precisión 0.4869\n",
            "Epoch 8 Lote 5750 Pérdida 0.9219 Precisión 0.4867\n",
            "Epoch 8 Lote 5800 Pérdida 0.9240 Precisión 0.4864\n",
            "Epoch 8 Lote 5850 Pérdida 0.9260 Precisión 0.4860\n",
            "Epoch 8 Lote 5900 Pérdida 0.9280 Precisión 0.4857\n",
            "Epoch 8 Lote 5950 Pérdida 0.9297 Precisión 0.4853\n",
            "Epoch 8 Lote 6000 Pérdida 0.9317 Precisión 0.4850\n",
            "Epoch 8 Lote 6050 Pérdida 0.9333 Precisión 0.4847\n",
            "Epoch 8 Lote 6100 Pérdida 0.9349 Precisión 0.4844\n",
            "Epoch 8 Lote 6150 Pérdida 0.9366 Precisión 0.4840\n",
            "Epoch 8 Lote 6200 Pérdida 0.9380 Precisión 0.4838\n",
            "Epoch 8 Lote 6250 Pérdida 0.9396 Precisión 0.4835\n",
            "Epoch 8 Lote 6300 Pérdida 0.9412 Precisión 0.4832\n",
            "Epoch 8 Lote 6350 Pérdida 0.9427 Precisión 0.4829\n",
            "Epoch 8 Lote 6400 Pérdida 0.9441 Precisión 0.4826\n",
            "Guardando checkpoint para el epoch 8 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-8\n",
            "Tiempo que ha tardado 1 epoch: 1596.9727466106415 segs\n",
            "\n",
            "Inicio del epoch 9\n",
            "Epoch 9 Lote 0 Pérdida 1.1461 Precisión 0.4918\n",
            "Epoch 9 Lote 50 Pérdida 1.1300 Precisión 0.4559\n",
            "Epoch 9 Lote 100 Pérdida 1.1237 Precisión 0.4566\n",
            "Epoch 9 Lote 150 Pérdida 1.1208 Precisión 0.4559\n",
            "Epoch 9 Lote 200 Pérdida 1.1144 Precisión 0.4564\n",
            "Epoch 9 Lote 250 Pérdida 1.1202 Precisión 0.4561\n",
            "Epoch 9 Lote 300 Pérdida 1.1205 Precisión 0.4555\n",
            "Epoch 9 Lote 350 Pérdida 1.1178 Precisión 0.4558\n",
            "Epoch 9 Lote 400 Pérdida 1.1115 Precisión 0.4553\n",
            "Epoch 9 Lote 450 Pérdida 1.1108 Precisión 0.4553\n",
            "Epoch 9 Lote 500 Pérdida 1.1059 Precisión 0.4555\n",
            "Epoch 9 Lote 550 Pérdida 1.1054 Precisión 0.4552\n",
            "Epoch 9 Lote 600 Pérdida 1.1044 Precisión 0.4554\n",
            "Epoch 9 Lote 650 Pérdida 1.1039 Precisión 0.4548\n",
            "Epoch 9 Lote 700 Pérdida 1.1041 Precisión 0.4552\n",
            "Epoch 9 Lote 750 Pérdida 1.1015 Precisión 0.4557\n",
            "Epoch 9 Lote 800 Pérdida 1.0966 Precisión 0.4569\n",
            "Epoch 9 Lote 850 Pérdida 1.0900 Precisión 0.4580\n",
            "Epoch 9 Lote 900 Pérdida 1.0846 Precisión 0.4594\n",
            "Epoch 9 Lote 950 Pérdida 1.0776 Precisión 0.4603\n",
            "Epoch 9 Lote 1000 Pérdida 1.0701 Precisión 0.4616\n",
            "Epoch 9 Lote 1050 Pérdida 1.0647 Precisión 0.4623\n",
            "Epoch 9 Lote 1100 Pérdida 1.0576 Precisión 0.4630\n",
            "Epoch 9 Lote 1150 Pérdida 1.0511 Precisión 0.4637\n",
            "Epoch 9 Lote 1200 Pérdida 1.0456 Precisión 0.4647\n",
            "Epoch 9 Lote 1250 Pérdida 1.0400 Precisión 0.4654\n",
            "Epoch 9 Lote 1300 Pérdida 1.0336 Precisión 0.4663\n",
            "Epoch 9 Lote 1350 Pérdida 1.0276 Precisión 0.4671\n",
            "Epoch 9 Lote 1400 Pérdida 1.0226 Precisión 0.4681\n",
            "Epoch 9 Lote 1450 Pérdida 1.0181 Precisión 0.4688\n",
            "Epoch 9 Lote 1500 Pérdida 1.0133 Precisión 0.4695\n",
            "Epoch 9 Lote 1550 Pérdida 1.0079 Precisión 0.4704\n",
            "Epoch 9 Lote 1600 Pérdida 1.0034 Precisión 0.4711\n",
            "Epoch 9 Lote 1650 Pérdida 0.9992 Precisión 0.4718\n",
            "Epoch 9 Lote 1700 Pérdida 0.9946 Precisión 0.4725\n",
            "Epoch 9 Lote 1750 Pérdida 0.9897 Precisión 0.4732\n",
            "Epoch 9 Lote 1800 Pérdida 0.9858 Precisión 0.4736\n",
            "Epoch 9 Lote 1850 Pérdida 0.9816 Precisión 0.4741\n",
            "Epoch 9 Lote 1900 Pérdida 0.9770 Precisión 0.4744\n",
            "Epoch 9 Lote 1950 Pérdida 0.9733 Precisión 0.4748\n",
            "Epoch 9 Lote 2000 Pérdida 0.9699 Precisión 0.4751\n",
            "Epoch 9 Lote 2050 Pérdida 0.9661 Precisión 0.4755\n",
            "Epoch 9 Lote 2100 Pérdida 0.9623 Precisión 0.4760\n",
            "Epoch 9 Lote 2150 Pérdida 0.9592 Precisión 0.4764\n",
            "Epoch 9 Lote 2200 Pérdida 0.9555 Precisión 0.4768\n",
            "Epoch 9 Lote 2250 Pérdida 0.9518 Precisión 0.4773\n",
            "Epoch 9 Lote 2300 Pérdida 0.9486 Precisión 0.4777\n",
            "Epoch 9 Lote 2350 Pérdida 0.9455 Precisión 0.4781\n",
            "Epoch 9 Lote 2400 Pérdida 0.9421 Precisión 0.4784\n",
            "Epoch 9 Lote 2450 Pérdida 0.9391 Precisión 0.4789\n",
            "Epoch 9 Lote 2500 Pérdida 0.9356 Precisión 0.4792\n",
            "Epoch 9 Lote 2550 Pérdida 0.9321 Precisión 0.4796\n",
            "Epoch 9 Lote 2600 Pérdida 0.9285 Precisión 0.4800\n",
            "Epoch 9 Lote 2650 Pérdida 0.9257 Precisión 0.4803\n",
            "Epoch 9 Lote 2700 Pérdida 0.9232 Precisión 0.4807\n",
            "Epoch 9 Lote 2750 Pérdida 0.9211 Precisión 0.4809\n",
            "Epoch 9 Lote 2800 Pérdida 0.9188 Precisión 0.4813\n",
            "Epoch 9 Lote 2850 Pérdida 0.9165 Precisión 0.4815\n",
            "Epoch 9 Lote 2900 Pérdida 0.9145 Precisión 0.4818\n",
            "Epoch 9 Lote 2950 Pérdida 0.9123 Precisión 0.4821\n",
            "Epoch 9 Lote 3000 Pérdida 0.9105 Precisión 0.4824\n",
            "Epoch 9 Lote 3050 Pérdida 0.9089 Precisión 0.4828\n",
            "Epoch 9 Lote 3100 Pérdida 0.9067 Precisión 0.4832\n",
            "Epoch 9 Lote 3150 Pérdida 0.9055 Precisión 0.4835\n",
            "Epoch 9 Lote 3200 Pérdida 0.9043 Precisión 0.4838\n",
            "Epoch 9 Lote 3250 Pérdida 0.9027 Precisión 0.4840\n",
            "Epoch 9 Lote 3300 Pérdida 0.9017 Precisión 0.4845\n",
            "Epoch 9 Lote 3350 Pérdida 0.9006 Precisión 0.4848\n",
            "Epoch 9 Lote 3400 Pérdida 0.8997 Precisión 0.4851\n",
            "Epoch 9 Lote 3450 Pérdida 0.8987 Precisión 0.4855\n",
            "Epoch 9 Lote 3500 Pérdida 0.8975 Precisión 0.4859\n",
            "Epoch 9 Lote 3550 Pérdida 0.8961 Precisión 0.4861\n",
            "Epoch 9 Lote 3600 Pérdida 0.8949 Precisión 0.4863\n",
            "Epoch 9 Lote 3650 Pérdida 0.8934 Precisión 0.4867\n",
            "Epoch 9 Lote 3700 Pérdida 0.8920 Precisión 0.4869\n",
            "Epoch 9 Lote 3750 Pérdida 0.8906 Precisión 0.4872\n",
            "Epoch 9 Lote 3800 Pérdida 0.8893 Precisión 0.4875\n",
            "Epoch 9 Lote 3850 Pérdida 0.8880 Precisión 0.4878\n",
            "Epoch 9 Lote 3900 Pérdida 0.8866 Precisión 0.4882\n",
            "Epoch 9 Lote 3950 Pérdida 0.8854 Precisión 0.4885\n",
            "Epoch 9 Lote 4000 Pérdida 0.8842 Precisión 0.4889\n",
            "Epoch 9 Lote 4050 Pérdida 0.8830 Precisión 0.4892\n",
            "Epoch 9 Lote 4100 Pérdida 0.8818 Precisión 0.4896\n",
            "Epoch 9 Lote 4150 Pérdida 0.8802 Precisión 0.4900\n",
            "Epoch 9 Lote 4200 Pérdida 0.8788 Precisión 0.4904\n",
            "Epoch 9 Lote 4250 Pérdida 0.8773 Precisión 0.4909\n",
            "Epoch 9 Lote 4300 Pérdida 0.8761 Precisión 0.4912\n",
            "Epoch 9 Lote 4350 Pérdida 0.8750 Precisión 0.4916\n",
            "Epoch 9 Lote 4400 Pérdida 0.8737 Precisión 0.4921\n",
            "Epoch 9 Lote 4450 Pérdida 0.8727 Precisión 0.4924\n",
            "Epoch 9 Lote 4500 Pérdida 0.8715 Precisión 0.4928\n",
            "Epoch 9 Lote 4550 Pérdida 0.8704 Precisión 0.4931\n",
            "Epoch 9 Lote 4600 Pérdida 0.8692 Precisión 0.4935\n",
            "Epoch 9 Lote 4650 Pérdida 0.8682 Precisión 0.4938\n",
            "Epoch 9 Lote 4700 Pérdida 0.8678 Precisión 0.4940\n",
            "Epoch 9 Lote 4750 Pérdida 0.8680 Precisión 0.4940\n",
            "Epoch 9 Lote 4800 Pérdida 0.8687 Precisión 0.4940\n",
            "Epoch 9 Lote 4850 Pérdida 0.8701 Precisión 0.4939\n",
            "Epoch 9 Lote 4900 Pérdida 0.8717 Precisión 0.4938\n",
            "Epoch 9 Lote 4950 Pérdida 0.8733 Precisión 0.4937\n",
            "Epoch 9 Lote 5000 Pérdida 0.8750 Precisión 0.4935\n",
            "Epoch 9 Lote 5050 Pérdida 0.8768 Precisión 0.4933\n",
            "Epoch 9 Lote 5100 Pérdida 0.8790 Precisión 0.4930\n",
            "Epoch 9 Lote 5150 Pérdida 0.8808 Precisión 0.4927\n",
            "Epoch 9 Lote 5200 Pérdida 0.8828 Precisión 0.4925\n",
            "Epoch 9 Lote 5250 Pérdida 0.8850 Precisión 0.4922\n",
            "Epoch 9 Lote 5300 Pérdida 0.8873 Precisión 0.4919\n",
            "Epoch 9 Lote 5350 Pérdida 0.8897 Precisión 0.4916\n",
            "Epoch 9 Lote 5400 Pérdida 0.8920 Precisión 0.4913\n",
            "Epoch 9 Lote 5450 Pérdida 0.8939 Precisión 0.4910\n",
            "Epoch 9 Lote 5500 Pérdida 0.8960 Precisión 0.4908\n",
            "Epoch 9 Lote 5550 Pérdida 0.8980 Precisión 0.4905\n",
            "Epoch 9 Lote 5600 Pérdida 0.9003 Precisión 0.4902\n",
            "Epoch 9 Lote 5650 Pérdida 0.9024 Precisión 0.4899\n",
            "Epoch 9 Lote 5700 Pérdida 0.9044 Precisión 0.4896\n",
            "Epoch 9 Lote 5750 Pérdida 0.9064 Precisión 0.4893\n",
            "Epoch 9 Lote 5800 Pérdida 0.9082 Precisión 0.4890\n",
            "Epoch 9 Lote 5850 Pérdida 0.9102 Precisión 0.4887\n",
            "Epoch 9 Lote 5900 Pérdida 0.9121 Precisión 0.4884\n",
            "Epoch 9 Lote 5950 Pérdida 0.9140 Precisión 0.4880\n",
            "Epoch 9 Lote 6000 Pérdida 0.9158 Precisión 0.4876\n",
            "Epoch 9 Lote 6050 Pérdida 0.9177 Precisión 0.4873\n",
            "Epoch 9 Lote 6100 Pérdida 0.9194 Precisión 0.4870\n",
            "Epoch 9 Lote 6150 Pérdida 0.9209 Precisión 0.4868\n",
            "Epoch 9 Lote 6200 Pérdida 0.9224 Precisión 0.4865\n",
            "Epoch 9 Lote 6250 Pérdida 0.9238 Precisión 0.4861\n",
            "Epoch 9 Lote 6300 Pérdida 0.9254 Precisión 0.4858\n",
            "Epoch 9 Lote 6350 Pérdida 0.9270 Precisión 0.4855\n",
            "Epoch 9 Lote 6400 Pérdida 0.9282 Precisión 0.4852\n",
            "Guardando checkpoint para el epoch 9 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-9\n",
            "Tiempo que ha tardado 1 epoch: 1600.5086560249329 segs\n",
            "\n",
            "Inicio del epoch 10\n",
            "Epoch 10 Lote 0 Pérdida 1.0560 Precisión 0.4844\n",
            "Epoch 10 Lote 50 Pérdida 1.1411 Precisión 0.4537\n",
            "Epoch 10 Lote 100 Pérdida 1.1236 Precisión 0.4542\n",
            "Epoch 10 Lote 150 Pérdida 1.1083 Precisión 0.4565\n",
            "Epoch 10 Lote 200 Pérdida 1.1112 Precisión 0.4565\n",
            "Epoch 10 Lote 250 Pérdida 1.1082 Precisión 0.4562\n",
            "Epoch 10 Lote 300 Pérdida 1.1054 Precisión 0.4573\n",
            "Epoch 10 Lote 350 Pérdida 1.1002 Precisión 0.4573\n",
            "Epoch 10 Lote 400 Pérdida 1.0985 Precisión 0.4578\n",
            "Epoch 10 Lote 450 Pérdida 1.0959 Precisión 0.4577\n",
            "Epoch 10 Lote 500 Pérdida 1.0934 Precisión 0.4579\n",
            "Epoch 10 Lote 550 Pérdida 1.0909 Precisión 0.4573\n",
            "Epoch 10 Lote 600 Pérdida 1.0891 Precisión 0.4571\n",
            "Epoch 10 Lote 650 Pérdida 1.0904 Precisión 0.4570\n",
            "Epoch 10 Lote 700 Pérdida 1.0868 Precisión 0.4573\n",
            "Epoch 10 Lote 750 Pérdida 1.0838 Precisión 0.4580\n",
            "Epoch 10 Lote 800 Pérdida 1.0800 Precisión 0.4588\n",
            "Epoch 10 Lote 850 Pérdida 1.0755 Precisión 0.4601\n",
            "Epoch 10 Lote 900 Pérdida 1.0704 Precisión 0.4612\n",
            "Epoch 10 Lote 950 Pérdida 1.0621 Precisión 0.4626\n",
            "Epoch 10 Lote 1000 Pérdida 1.0562 Precisión 0.4638\n",
            "Epoch 10 Lote 1050 Pérdida 1.0486 Precisión 0.4647\n",
            "Epoch 10 Lote 1100 Pérdida 1.0431 Precisión 0.4657\n",
            "Epoch 10 Lote 1150 Pérdida 1.0371 Precisión 0.4663\n",
            "Epoch 10 Lote 1200 Pérdida 1.0323 Precisión 0.4670\n",
            "Epoch 10 Lote 1250 Pérdida 1.0264 Precisión 0.4677\n",
            "Epoch 10 Lote 1300 Pérdida 1.0204 Precisión 0.4685\n",
            "Epoch 10 Lote 1350 Pérdida 1.0136 Precisión 0.4694\n",
            "Epoch 10 Lote 1400 Pérdida 1.0082 Precisión 0.4702\n",
            "Epoch 10 Lote 1450 Pérdida 1.0025 Precisión 0.4711\n",
            "Epoch 10 Lote 1500 Pérdida 0.9978 Precisión 0.4718\n",
            "Epoch 10 Lote 1550 Pérdida 0.9922 Precisión 0.4724\n",
            "Epoch 10 Lote 1600 Pérdida 0.9878 Precisión 0.4734\n",
            "Epoch 10 Lote 1650 Pérdida 0.9835 Precisión 0.4741\n",
            "Epoch 10 Lote 1700 Pérdida 0.9790 Precisión 0.4749\n",
            "Epoch 10 Lote 1750 Pérdida 0.9749 Precisión 0.4755\n",
            "Epoch 10 Lote 1800 Pérdida 0.9706 Precisión 0.4761\n",
            "Epoch 10 Lote 1850 Pérdida 0.9668 Precisión 0.4765\n",
            "Epoch 10 Lote 1900 Pérdida 0.9629 Precisión 0.4769\n",
            "Epoch 10 Lote 1950 Pérdida 0.9590 Precisión 0.4772\n",
            "Epoch 10 Lote 2000 Pérdida 0.9553 Precisión 0.4777\n",
            "Epoch 10 Lote 2050 Pérdida 0.9515 Precisión 0.4781\n",
            "Epoch 10 Lote 2100 Pérdida 0.9477 Precisión 0.4786\n",
            "Epoch 10 Lote 2150 Pérdida 0.9441 Precisión 0.4790\n",
            "Epoch 10 Lote 2200 Pérdida 0.9408 Precisión 0.4795\n",
            "Epoch 10 Lote 2250 Pérdida 0.9374 Precisión 0.4797\n",
            "Epoch 10 Lote 2300 Pérdida 0.9343 Precisión 0.4802\n",
            "Epoch 10 Lote 2350 Pérdida 0.9304 Precisión 0.4806\n",
            "Epoch 10 Lote 2400 Pérdida 0.9270 Precisión 0.4809\n",
            "Epoch 10 Lote 2450 Pérdida 0.9235 Precisión 0.4813\n",
            "Epoch 10 Lote 2500 Pérdida 0.9203 Precisión 0.4816\n",
            "Epoch 10 Lote 2550 Pérdida 0.9170 Precisión 0.4819\n",
            "Epoch 10 Lote 2600 Pérdida 0.9139 Precisión 0.4821\n",
            "Epoch 10 Lote 2650 Pérdida 0.9111 Precisión 0.4825\n",
            "Epoch 10 Lote 2700 Pérdida 0.9091 Precisión 0.4828\n",
            "Epoch 10 Lote 2750 Pérdida 0.9066 Precisión 0.4832\n",
            "Epoch 10 Lote 2800 Pérdida 0.9044 Precisión 0.4835\n",
            "Epoch 10 Lote 2850 Pérdida 0.9024 Precisión 0.4840\n",
            "Epoch 10 Lote 2900 Pérdida 0.9005 Precisión 0.4843\n",
            "Epoch 10 Lote 2950 Pérdida 0.8987 Precisión 0.4846\n",
            "Epoch 10 Lote 3000 Pérdida 0.8970 Precisión 0.4849\n",
            "Epoch 10 Lote 3050 Pérdida 0.8955 Precisión 0.4852\n",
            "Epoch 10 Lote 3100 Pérdida 0.8940 Precisión 0.4855\n",
            "Epoch 10 Lote 3150 Pérdida 0.8928 Precisión 0.4858\n",
            "Epoch 10 Lote 3200 Pérdida 0.8914 Precisión 0.4861\n",
            "Epoch 10 Lote 3250 Pérdida 0.8897 Precisión 0.4864\n",
            "Epoch 10 Lote 3300 Pérdida 0.8881 Precisión 0.4868\n",
            "Epoch 10 Lote 3350 Pérdida 0.8866 Precisión 0.4871\n",
            "Epoch 10 Lote 3400 Pérdida 0.8856 Precisión 0.4873\n",
            "Epoch 10 Lote 3450 Pérdida 0.8843 Precisión 0.4877\n",
            "Epoch 10 Lote 3500 Pérdida 0.8833 Precisión 0.4881\n",
            "Epoch 10 Lote 3550 Pérdida 0.8820 Precisión 0.4883\n",
            "Epoch 10 Lote 3600 Pérdida 0.8807 Precisión 0.4886\n",
            "Epoch 10 Lote 3650 Pérdida 0.8792 Precisión 0.4889\n",
            "Epoch 10 Lote 3700 Pérdida 0.8780 Precisión 0.4892\n",
            "Epoch 10 Lote 3750 Pérdida 0.8767 Precisión 0.4895\n",
            "Epoch 10 Lote 3800 Pérdida 0.8755 Precisión 0.4898\n",
            "Epoch 10 Lote 3850 Pérdida 0.8742 Precisión 0.4901\n",
            "Epoch 10 Lote 3900 Pérdida 0.8728 Precisión 0.4904\n",
            "Epoch 10 Lote 3950 Pérdida 0.8715 Precisión 0.4908\n",
            "Epoch 10 Lote 4000 Pérdida 0.8702 Precisión 0.4912\n",
            "Epoch 10 Lote 4050 Pérdida 0.8691 Precisión 0.4915\n",
            "Epoch 10 Lote 4100 Pérdida 0.8678 Precisión 0.4919\n",
            "Epoch 10 Lote 4150 Pérdida 0.8665 Precisión 0.4923\n",
            "Epoch 10 Lote 4200 Pérdida 0.8647 Precisión 0.4927\n",
            "Epoch 10 Lote 4250 Pérdida 0.8631 Precisión 0.4932\n",
            "Epoch 10 Lote 4300 Pérdida 0.8618 Precisión 0.4935\n",
            "Epoch 10 Lote 4350 Pérdida 0.8609 Precisión 0.4939\n",
            "Epoch 10 Lote 4400 Pérdida 0.8597 Precisión 0.4943\n",
            "Epoch 10 Lote 4450 Pérdida 0.8584 Precisión 0.4946\n",
            "Epoch 10 Lote 4500 Pérdida 0.8572 Precisión 0.4951\n",
            "Epoch 10 Lote 4550 Pérdida 0.8562 Precisión 0.4955\n",
            "Epoch 10 Lote 4600 Pérdida 0.8550 Precisión 0.4959\n",
            "Epoch 10 Lote 4650 Pérdida 0.8542 Precisión 0.4962\n",
            "Epoch 10 Lote 4700 Pérdida 0.8538 Precisión 0.4964\n",
            "Epoch 10 Lote 4750 Pérdida 0.8539 Precisión 0.4965\n",
            "Epoch 10 Lote 4800 Pérdida 0.8545 Precisión 0.4965\n",
            "Epoch 10 Lote 4850 Pérdida 0.8558 Precisión 0.4964\n",
            "Epoch 10 Lote 4900 Pérdida 0.8572 Precisión 0.4963\n",
            "Epoch 10 Lote 4950 Pérdida 0.8589 Precisión 0.4961\n",
            "Epoch 10 Lote 5000 Pérdida 0.8608 Precisión 0.4958\n",
            "Epoch 10 Lote 5050 Pérdida 0.8628 Precisión 0.4956\n",
            "Epoch 10 Lote 5100 Pérdida 0.8650 Precisión 0.4953\n",
            "Epoch 10 Lote 5150 Pérdida 0.8669 Precisión 0.4950\n",
            "Epoch 10 Lote 5200 Pérdida 0.8690 Precisión 0.4948\n",
            "Epoch 10 Lote 5250 Pérdida 0.8710 Precisión 0.4944\n",
            "Epoch 10 Lote 5300 Pérdida 0.8734 Precisión 0.4942\n",
            "Epoch 10 Lote 5350 Pérdida 0.8754 Precisión 0.4938\n",
            "Epoch 10 Lote 5400 Pérdida 0.8777 Precisión 0.4936\n",
            "Epoch 10 Lote 5450 Pérdida 0.8798 Precisión 0.4933\n",
            "Epoch 10 Lote 5500 Pérdida 0.8818 Precisión 0.4930\n",
            "Epoch 10 Lote 5550 Pérdida 0.8839 Precisión 0.4927\n",
            "Epoch 10 Lote 5600 Pérdida 0.8857 Precisión 0.4924\n",
            "Epoch 10 Lote 5650 Pérdida 0.8878 Precisión 0.4921\n",
            "Epoch 10 Lote 5700 Pérdida 0.8897 Precisión 0.4918\n",
            "Epoch 10 Lote 5750 Pérdida 0.8917 Precisión 0.4915\n",
            "Epoch 10 Lote 5800 Pérdida 0.8937 Precisión 0.4912\n",
            "Epoch 10 Lote 5850 Pérdida 0.8958 Precisión 0.4909\n",
            "Epoch 10 Lote 5900 Pérdida 0.8977 Precisión 0.4905\n",
            "Epoch 10 Lote 5950 Pérdida 0.8996 Precisión 0.4902\n",
            "Epoch 10 Lote 6000 Pérdida 0.9014 Precisión 0.4899\n",
            "Epoch 10 Lote 6050 Pérdida 0.9032 Precisión 0.4895\n",
            "Epoch 10 Lote 6100 Pérdida 0.9048 Precisión 0.4892\n",
            "Epoch 10 Lote 6150 Pérdida 0.9064 Precisión 0.4889\n",
            "Epoch 10 Lote 6200 Pérdida 0.9080 Precisión 0.4886\n",
            "Epoch 10 Lote 6250 Pérdida 0.9096 Precisión 0.4883\n",
            "Epoch 10 Lote 6300 Pérdida 0.9110 Precisión 0.4880\n",
            "Epoch 10 Lote 6350 Pérdida 0.9126 Precisión 0.4877\n",
            "Epoch 10 Lote 6400 Pérdida 0.9140 Precisión 0.4874\n",
            "Guardando checkpoint para el epoch 10 en ./drive/My Drive/Curso de NLP/Transformer/ckpt/ckpt-10\n",
            "Tiempo que ha tardado 1 epoch: 1607.4231536388397 segs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmzyRwDrRGdq",
        "colab_type": "text"
      },
      "source": [
        "# Evaluación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNHwJJrz3lPB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_ES-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions, attention_weights = transformer(enc_input, output, False) #(1, seq_length, VOCAB_SIZE_ES)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_ES-1:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaRt6Vl9LptL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "    sentence = tokenizer_en.encode(sentence)\n",
        "\n",
        "    attention = tf.squeeze(attention[layer], axis=0)\n",
        "\n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(2, 4, head+1)\n",
        "    \n",
        "        ax.matshow(attention[head][:-1, :])\n",
        "\n",
        "        fontdict = {'fontsize': 10,\n",
        "                    'color': 'black'} \n",
        "    \n",
        "        ax.set_xticks(range(len(sentence)))\n",
        "        ax.set_yticks(range(len(result)))\n",
        "    \n",
        "        ax.set_ylim(len(result)-1.5, -0.5)\n",
        "        \n",
        "        ax.set_xticklabels(\n",
        "            ['<start>']+[tokenizer_en.decode([i]) for i in sentence]+['<end>'], \n",
        "            fontdict=fontdict, rotation=90)\n",
        "    \n",
        "        ax.set_yticklabels([tokenizer_es.decode([i]) for i in result \n",
        "                            if i < tokenizer_es.vocab_size], \n",
        "                           fontdict=fontdict)\n",
        "    \n",
        "        ax.set_xlabel('Cabecera {}'.format(head+1))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6VeFKrE6Kdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence, plot=''):\n",
        "    output, attention_weights = evaluate(sentence)\n",
        "    \n",
        "    predicted_sentence = tokenizer_es.decode(\n",
        "        [i for i in output.numpy() if i < VOCAB_SIZE_ES-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Entrada: {}\".format(sentence))\n",
        "    print(\"Traducción predicha: {}\".format(predicted_sentence))\n",
        "    \n",
        "    if plot:\n",
        "        plot_attention_weights(attention_weights, sentence, output, plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BupFjJlgDvCA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "b8e4c044-eb24-46ba-be33-89158885f6ca"
      },
      "source": [
        "translate(\"I do not know whether this information is correct\", plot=\"decoder_layer_4_cross\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entrada: I do not know whether this information is correct\n",
            "Traducción predicha: No sé si esta información es correcta.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x576 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq1lu1FMQ3cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}